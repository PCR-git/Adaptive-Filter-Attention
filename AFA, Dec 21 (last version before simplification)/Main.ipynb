{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55sKZVe2NcuJ"
   },
   "source": [
    "## Adaptive Filter Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1IgNYUyROjm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.rc('font', size=20)\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import transformers\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from collections import Counter\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm # Loading bar\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import complex_exp, complex_hadamard, complex_matmul\n",
    "from utils import batched_complex_exp, batched_complex_hadamard, batched_complex_matmul\n",
    "from utils import batched_complex_hadamard_full, batched_complex_matmul_full\n",
    "from utils import count_parameters, get_layers\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamics import stochastic_LTI, DynamicSim\n",
    "from dynamics import construct_mapping\n",
    "from dynamics import get_nth_measurement, get_random_measurements\n",
    "from dynamics import linear_spiral, linear_spiral_3D, Lorenz, rand_coupling_matrix, Van_der_Pol_osc\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isotropic_afa import get_safe_exp_tot, compute_covariance_matrix, compute_covariance_matrix_safe\n",
    "from isotropic_afa import compute_exp_kernel_isotropic, compute_exp_kernel_anisotropic\n",
    "from isotropic_afa import build_factorized_kernels, compute_residual_norm_isotropic, compute_residual_norm_anisotropic\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import compute_lambda, resolve_multihead_dims\n",
    "from model import init_complexlinear, init_complex_matrix, set_complex_weight, initialize_linear_layers\n",
    "from model import initialize_to_correct_model\n",
    "from model import apply_weight_masks\n",
    "from model import ComplexLinearLayer, ComplexLinearHermitianLayer, ComplextoRealLinearLayer\n",
    "from model import MultiHeadAttentionLayer\n",
    "from model import ComplexRMSNorm\n",
    "from model import Attention_1layer, AFA_1layer\n",
    "from model import SimpleAttention_Net\n",
    "from model import TransformerBlock, TransformerNetwork\n",
    "from model import MultiheadIsotropicAFA_1layer, AFATransformerBlock, AFATransformerNetwork\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import construct_random_mapping, construct_data_dynamics, TrainDataset_dynamics, create_train_loader_dynamics\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import plot_trajectory, compute_state_matrix, plot_state_matrix, plot_eigenvals, visualize_results\n",
    "from visualization import visualize_results_attn, _get_visual_modules\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import single_iter, single_epoch, hook_fn\n",
    "from training import single_iter_attn, single_epoch_attn\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeWE9GM-R_40"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser('DA')\n",
    "parser.add_argument('--gpu', type=int, default=0) # (Default: 0)\n",
    "args = parser.parse_args(args=[])\n",
    "args.device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
    "print(args.device)\n",
    "\n",
    "torch.manual_seed(2025)\n",
    "np.random.seed(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dynamical system\n",
    "\n",
    "# # Normal system\n",
    "# D1 = torch.zeros(2,2,2).to(args.device) # Diagonal matrix\n",
    "# D1[0] = torch.tensor([[-0.1, 0.0], [0.0, -0.1]]).to(args.device)\n",
    "# D1[1] = torch.tensor([[-1.0, 0.0], [0.0, 1.0]]).to(args.device)\n",
    "# # S1 = torch.zeros(2,2,2).to(args.device)\n",
    "# # S1[0] = torch.tensor(([1.0,1.0],[0.0,0.0]))\n",
    "# # S1[1] = torch.tensor(([0.0,0.0],[1.0,-1.0]))\n",
    "# # S1 = U/np.sqrt(2)\n",
    "# alpha = np.random.uniform(low=0.0, high=1.0)*2*np.pi\n",
    "# beta = np.random.uniform(low=0.0, high=1.0)*2*np.pi\n",
    "# S1 = construct_special_2D_unitary(alpha=alpha, beta=beta)\n",
    "# Si1 = complex_conj_transpose(S1)\n",
    "\n",
    "# Stable 2D linear system (in diagonalized form):\n",
    "D1 = torch.zeros(2,2,2).to(args.device) # Diagonal matrix\n",
    "S1 = torch.zeros(2,2,2).to(args.device) # RHS matrix\n",
    "Si1 = torch.zeros(2,2,2).to(args.device) # Inverse of RHS matrix\n",
    "D1[0] = torch.tensor([[-0.1, 0.0], [0.0, -0.1]]).to(args.device) # Negative real part eigenvals\n",
    "# D1[0] = torch.tensor([[0.0, 0.0], [0.0, 0.0]]).to(args.device) # Zero real part eigenvals\n",
    "# D1[0] = torch.tensor([[0.1, 0.0], [0.0, 0.1]]).to(args.device) # Positive real part eigenvals\n",
    "D1[1] = torch.tensor([[-1.0, 0.0], [0.0, 1.0]]).to(args.device)\n",
    "S1[0] = torch.tensor([[1.0, 1.0], [1.0, 1.0]]).to(args.device)\n",
    "S1[1] = torch.tensor([[-1.0, 1.0], [0.0, 0.0]]).to(args.device)\n",
    "Si1[0] = 0.5*torch.tensor([[0.0, 1.0], [0.0, 1.0]]).to(args.device)\n",
    "Si1[1] = 0.5*torch.tensor([[1.0, -1.0], [-1.0, 1.0]]).to(args.device)\n",
    "A = complex_matmul(S1,complex_matmul(D1,Si1))[0].unsqueeze(0)\n",
    "params = [D1, S1, Si1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7F6aLjrv8-gS",
    "outputId": "fbf50e25-b410-4406-a22f-4543fe725e59"
   },
   "outputs": [],
   "source": [
    "# DEFINE MODEL\n",
    "\n",
    "args.cr_max = 2\n",
    "args.cr_min = 0\n",
    "\n",
    "args.max_exponent = 10\n",
    "args.min_exponent = -10\n",
    "\n",
    "args.t_equal = 1 # Equal time intervals? (0 or 1)\n",
    "args.sep_params = 0 # Use separate params for keys and values? (0 or 1)\n",
    "\n",
    "##################################################\n",
    "## Testing options\n",
    "args.rand_embed = 0 # Use random orthogonal matrix when projecting data to higher dimension?\n",
    "args.weight_mask = 0 # Mask the weights? (0 or 1) (must be set to 1 when visualizing in 2D)\n",
    "args.lambda_real_zero = 0 # Force real part of lambda to be zero? (0=No, 1=Yes)\n",
    "##################################################\n",
    "\n",
    "##################################################\n",
    "## Ablation options ##\n",
    "######################\n",
    "## (Setting everything to 0 (except lambda_real_zero & outer residual norm) makes it an ordinary Transformer)\n",
    "args.lambda_real_zero = 0 # Zero out real part of eigenvalues? (1 = yes, 0 = no)\n",
    "args.use_full_residual_norm = 1 # Use the full |R|^2 metric and rational robust weight (1) or dot product / exponential weight (0)?\n",
    "args.use_robust_weight = 1 # Use rational weight rather than softmax\n",
    "args.additive_bias_type = 1 # (Additive bias: 0 for zero; 1 for DLE; 2 for linear)\n",
    "args.multiplicative_bias_type = 1 # (Multiplicative bias: 0 for constant; 1 for DLE; 2 for linear)\n",
    "args.compute_next_step_pred = 1 # Compute next-step prediction? (1 = yes, 0 = No)\n",
    "args.learn_rotations = 1 # Learned rotations (1), or fixed as in RoPE (0)?\n",
    "args.rotate_values = 1 # Rotate/unrotate values? (1 = yes, 0 = No)\n",
    "args.zero_process_noise = 0 # Zero process noise (sigma^2)? (1 = yes, 0 = No)\n",
    "args.zero_key_measurement_noise = 0 # Zero key measurement noise (eta^2)? (1 = yes, 0 = No)\n",
    "args.use_total_precision_gate = 0 # Use total-precision gating? (0 = No gate, 2 = precision gate, 2 = learned gate)\n",
    "args.use_complex_input_norm = 0 # Use complex-valued RMS Norm AFTER input projection (1) or real-valued RMS Norm BEFORE input projection (0)?\n",
    "args.use_inner_residual_and_norm = 0\n",
    "# args.use_complex_output_norm = 1\n",
    "# args.use_inner_residual = 1\n",
    "# args.use_outer_residual = 0\n",
    "args.add_gaussian_noise = 0 # Add Gaussian noise (for test-time sampling)\n",
    "\n",
    "##################################################\n",
    "\n",
    "args.compute_next_step_pred = 1\n",
    "\n",
    "args.m = 2 # Dimension of simulated system\n",
    "# args.embed_dim = 2 # Embedding dimension\n",
    "args.d_e = 128 # Embedding dimension\n",
    "\n",
    "# args.num_heads = 1\n",
    "# args.head_dim = int(args.embed_dim/args.num_heads)\n",
    "\n",
    "##################################################\n",
    "\n",
    "# Key, query, value embedding dimensions are same as input embedding dimension\n",
    "args.d_k = args.d_e\n",
    "args.d_v = args.d_e\n",
    "# args.d_k = args.head_dim\n",
    "# args.d_v = args.head_dim\n",
    "\n",
    "# # Key, query, value embedding dimensions are half of input embedding dimension\n",
    "# args.d_k = int(args.head_dim/2) # Key and query embedding dimension\n",
    "# args.d_v = int(args.head_dim/2) # Value embedding dimension\n",
    "\n",
    "# # Key, query, value embedding dimensions are 2\n",
    "# args.d_k = 2 # Key and query embedding dimension\n",
    "# args.d_v = 2 # Value embedding dimension\n",
    "\n",
    "##################################################\n",
    "\n",
    "args.nu = 1.0 # Measurement weighting\n",
    "args.tf = 10.0 # Final time\n",
    "args.dt = 0.01 # Time step size\n",
    "args.n = 10 # nth measurement (ie use every nth point as a measurement)\n",
    "args.delta_t = args.dt * args.n\n",
    "\n",
    "args.N_t = int(args.tf/args.dt) # Number of time steps\n",
    "args.seq_len = int(args.N_t/args.n)+1 # Number of measurements\n",
    "# args.seq_len = args.N_t / args.n = (args.tf/args.dt) / (args.delta_t/args.dt) = args.tf/args.delta_t\n",
    "t_v = (torch.arange(args.N_t + args.n + 1)*args.dt).to(args.device) # Array of time steps\n",
    "\n",
    "# Some scalar weights in model\n",
    "args.alpha = 1.0\n",
    "args.beta = 0.0\n",
    "args.delta = 1.0\n",
    "args.eta = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE MODEL:\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# # Standard Attention/Transformer Models\n",
    "\n",
    "# args.model_type = 'RealInputs'\n",
    "# args.num_heads = 4\n",
    "# args.num_blocks = 3\n",
    "# # model = Attention_1layer(args.d_e, args.d_v, args.num_heads, args).to(args.device)\n",
    "# # model = SimpleAttention_Net(args.d_e, args.d_v*2, args.num_heads, args).to(args.device)\n",
    "# model = TransformerNetwork(args.d_e, args.d_v*2, args.d_v*4, args.num_heads, args, num_blocks=args.num_blocks).to(args.device)\n",
    "# loss = nn.MSELoss()\n",
    "# loss_p = nn.MSELoss()\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# # Complex Attention with Real Input/Output Models\n",
    "\n",
    "# args.model_type = 'RealInputs'\n",
    "# args.metric = 'RealDotProduct'\n",
    "# # args.metric_type = 'InverseMahalanobis'\n",
    "# # model = SimpleComplexAttention_Net(args.d_e, args.d_v, args).to(args.device)\n",
    "# model = ComplexTransformerNetwork(args.d_e, args.d_v*2, args.d_v*4, args, num_blocks=2).to(args.device)\n",
    "# loss = nn.MSELoss()\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# # Multihead Isotropic AFA\n",
    "\n",
    "# args.num_heads = 2 # Number of heads\n",
    "# # args.model_type = 'RealInputs' # Real-valued inputs\n",
    "# # args.num_inner_layers = 1 # Number of layers inside an Nlayer block\n",
    "# # self.args.compute_next_step_pred = 0\n",
    "# # args.d_k = args.d_e\n",
    "# # args.d_v = args.d_e\n",
    "# # model = MultiheadSimplifiedAFA_1layer(args, args.seq_len, args.num_heads, input_dim=args.d_e, query_key_dim=args.d_k, value_dim=args.d_v).to(args.device)\n",
    "# args.d_k_total = args.d_e # Total query-key dim across all heads\n",
    "# args.d_v_total = args.d_e # Total value dim across all heads\n",
    "# model = MultiheadIsotropicAFA_1layer(args, args.num_heads, input_dim=args.d_e, query_key_dim_total=args.d_k_total, value_dim_total=args.d_v_total).to(args.device)\n",
    "\n",
    "# # args.num_blocks = 1 # Number of transformer blocks\n",
    "# # model = MultiheadSimplifiedAFATransformerNetwork(args, args.seq_len, num_blocks=args.num_blocks, n_heads=args.num_heads, input_dim=args.d_e, query_key_dim_total=args.d_k_total, value_dim_total=args.d_v_total, Norm=Norm).to(args.device)\n",
    "\n",
    "# loss = nn.MSELoss() # Loss\n",
    "# loss_p = nn.MSELoss() # Penalty\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# AFA Transformer\n",
    "\n",
    "args.compute_next_step_pred = 0\n",
    "args.num_heads = 2 # Number of heads\n",
    "args.num_blocks = 2 # Number of transformer blocks\n",
    "args.d_k_total = args.d_e # Total query-key dim across all heads\n",
    "args.d_v_total = args.d_e # Total value dim across all heads\n",
    "model = AFATransformerNetwork(args=args, num_blocks=args.num_blocks, n_heads=args.num_heads, input_dim=args.d_e, query_key_dim_total=args.d_k, value_dim_total=args.d_v, hidden_dim = 4*args.d_v, Norm=nn.LayerNorm).to(args.device)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "loss_p = nn.MSELoss()\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = list(model.parameters()) # Parameters list\n",
    "\n",
    "# loss_p = Complex_Trace_Loss() # Trace Penalty\n",
    "lr = 1E-2 # Learning rate\n",
    "optimizer = torch.optim.Adam(params_list, lr=lr, betas=(0.9, 0.999)) # Optimizer\n",
    "\n",
    "if args.d_k != args.d_v and args.sep_params == 0:\n",
    "    print('ERROR: Key and value embedding dimensions must be the same if using shared parameters.')\n",
    "\n",
    "print('Total parameter count:', count_parameters(model))\n",
    "\n",
    "#####################\n",
    "\n",
    "# Create folders for model weights, and loss history\n",
    "\n",
    "saved_models_path = 'C:/Users/Pracioppo/Desktop/AFA, Dec 11/saved_models/'\n",
    "model_name = str(model.__class__.__name__)\n",
    "date = str(datetime.datetime.today()).split()[0]\n",
    "model_path = saved_models_path + model_name + '__' + date + '/'\n",
    "\n",
    "model_weight_path = model_path + 'model_weights/'\n",
    "model_tensor_path = model_path + 'info_tensors/'\n",
    "\n",
    "try:\n",
    "    os.mkdir(model_path)\n",
    "    os.mkdir(model_weight_path)\n",
    "    os.mkdir(model_tensor_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aJOYmJ79Wmi6",
    "outputId": "d306dcec-02cd-4168-b222-0f1b3bbbc1f7"
   },
   "outputs": [],
   "source": [
    "## CREATE TRAINING DATA\n",
    "\n",
    "args.concat_mag = 0\n",
    "\n",
    "# Training params\n",
    "args.num_epochs = 1000 # Number of epochs\n",
    "args.num_samp = 32 # Number of samples in train loader\n",
    "args.batch_size = 32 # Batch size\n",
    "args.num_its = int(args.num_samp/args.batch_size) # Number of iterations in an epoch\n",
    "args.save_epochs = 100 # Intervals of epochs to save model\n",
    "args.show_example_epochs = 5 # Number of epochs between displaying results so far\n",
    "args.n_example = 5 # Plot state estimates at n_example data points\n",
    "args.epsilon = 1E-5\n",
    "\n",
    "# Process and measurement noise:\n",
    "sigma_process = 0.0 # Process noise\n",
    "sigma_measure = 2.0 # Measurement noise\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# Initialize arrays to record losses\n",
    "mean_epoch_losses = np.zeros(args.num_epochs)\n",
    "log_mean_epoch_losses = np.zeros(args.num_epochs)\n",
    "all_losses = np.zeros(args.num_epochs * args.num_samp)\n",
    "all_lambdas = np.zeros((args.num_epochs, 2, args.d_v))\n",
    "\n",
    "Pu, Pd, R1, R1i = construct_random_mapping(S1, Si1, args) # Get random matrices\n",
    "\n",
    "# Build training dataset\n",
    "train_loader, train_dataset, X_true_all, X_measure_all, t_measure_all = create_train_loader_dynamics(A, S1, Si1, Pu, Pd, R1, R1i, t_v, sigma_process, sigma_measure, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT ALL TRAJECTORIES\n",
    "\n",
    "from matplotlib import patches\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for it, (train_data, X_true, X_measure, t_measure_full) in enumerate(train_loader):\n",
    "# for it, train_data in enumerate(train_loader):\n",
    "#     for i in range(args.batch_size):\n",
    "\n",
    "#     if X_true.size()[2] == 1:\n",
    "#         X_true = torch.stack((X_true, torch.zeros_like(X_true)),dim=2)\n",
    "#         X_measure = torch.stack((X_measure, torch.zeros_like(X_measure)),dim=2)\n",
    "\n",
    "    # Actual trajectory\n",
    "    X_true_plt = X_true.squeeze().detach().cpu().numpy()\n",
    "    plt.plot(X_true_plt.T[0],X_true_plt.T[1],'black')\n",
    "\n",
    "    # Noisy trajectory\n",
    "    traj = X_measure.detach().cpu().squeeze().numpy()\n",
    "    plt.plot(traj.T[0], traj.T[1], 'b--')\n",
    "    \n",
    "    x0 = X_true_plt[:,0].T\n",
    "    plt.scatter(x0[0],x0[1], color='red')\n",
    "\n",
    "#     circle = patches.Circle((0,0), 17.0, edgecolor='red', facecolor='none', linewidth = 2)\n",
    "#     ax.add_patch(circle)\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### Initialize model to correct values (for testing) ######\n",
    "# ##############################################################\n",
    "\n",
    "# if isinstance(model, MultiheadIsotropicAFA_1layer):\n",
    "#     module = model.layers[0]\n",
    "#     initialize_to_correct_model(module, D1, S1, Si1, sigma_process, sigma_measure, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  Load in model weights ######\n",
    "####################################\n",
    "\n",
    "# epoch = 410\n",
    "# model_weight_path_full = model_weight_path + 'weights_epoch_' + str(epoch)\n",
    "\n",
    "# try:\n",
    "#     model.load_state_dict(torch.load(model_weight_path_full, weights_only=True))\n",
    "#     print('Model loaded.')\n",
    "# except:\n",
    "#     print('Model loading failed.')\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### TRAINING LOOP ###########################\n",
    "#####################################################################\n",
    "\n",
    "###### LOOP FOR PRECISION ATTENTION LAYERS #######\n",
    "\n",
    "# Train for num_epochs:\n",
    "for epoch in tqdm(np.arange(args.num_epochs), desc=\"Training progress...\"):\n",
    "\n",
    "    # Train for single epoch\n",
    "    epoch_losses, A_hat_complex, epoch_lambdas, unnormalized_attention = single_epoch(model, train_loader, optimizer, loss, loss_p, params_list, args)\n",
    "\n",
    "    # Collect losses\n",
    "    all_losses[epoch*args.num_its:(epoch+1)*args.num_its] = epoch_losses\n",
    "    mean_epoch_losses[epoch] = np.mean(epoch_losses)\n",
    "    log_mean_epoch_losses[epoch] = np.log(np.mean(epoch_losses))\n",
    "    all_lambdas[epoch,:,:] = np.mean(epoch_lambdas,axis=0)\n",
    "    \n",
    "    # Visualize results so far:\n",
    "    if np.mod(epoch+1,args.show_example_epochs) == 0:\n",
    "        visualize_results(model, train_dataset, all_losses, mean_epoch_losses, log_mean_epoch_losses, all_lambdas, R1, R1i, Pu, Pd, A, epoch, args)\n",
    "\n",
    "#     if np.mod(epoch+1,args.save_epochs) == 0:\n",
    "#         model_weight_path_full = model_weight_path + 'weights_epoch_' + str(epoch)\n",
    "#         model_tensor_path_full = model_tensor_path + 'tensors_epoch_' + str(epoch)\n",
    "#         torch.save(model.state_dict(), model_weight_path_full)\n",
    "\n",
    "#         tensors_to_save = {\n",
    "#         'epoch_losses': epoch_losses,\n",
    "#         'A_hat_complex': A_hat_complex,\n",
    "#         'epoch_lambdas': epoch_lambdas\n",
    "#         }\n",
    "#         torch.save(tensors_to_save, model_tensor_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Model\n",
    "# model_weight_path_full = model_weight_path + 'weights_epoch_' + str(epoch)\n",
    "# model_tensor_path_full = model_tensor_path + 'tensors_epoch_' + str(epoch)\n",
    "# torch.save(model.state_dict(), model_weight_path_full)\n",
    "\n",
    "# tensors_to_save = {\n",
    "# 'epoch_losses': epoch_losses,\n",
    "# 'A_hat_complex': A_hat_complex,\n",
    "# 'epoch_lambdas': epoch_lambdas\n",
    "# }\n",
    "# torch.save(tensors_to_save, model_tensor_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### LOOP FOR REAL/COMPLEX ATTENTION NETS #######\n",
    "\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Training progress...\"):\n",
    "\n",
    "#     epoch_losses = single_epoch_attn(model, train_loader, optimizer, loss, params_list, args)\n",
    "\n",
    "#     # Collect losses\n",
    "#     all_losses[epoch*args.num_its:(epoch+1)*args.num_its] = epoch_losses\n",
    "#     log_mean_epoch_losses[epoch] = np.log(np.mean(epoch_losses))\n",
    "    \n",
    "#     # Visualize results so far:\n",
    "#     if np.mod(epoch+1,args.show_example_epochs) == 0:\n",
    "#         visualize_results_attn(model, train_dataset, all_losses, mean_epoch_losses, log_mean_epoch_losses, all_lambdas, R1, R1i, Pu, Pd, A, epoch, args)\n",
    "# ##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the model autoregressively\n",
    "def autoregressive_sample(model, start_seq, max_gen_len):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        current_seq = start_seq\n",
    "        total_seq = start_seq\n",
    "        \n",
    "        window_size = start_seq.size(2)\n",
    "\n",
    "        for i in range(max_gen_len):\n",
    "            # Forward pass through the model\n",
    "            out, output_dict = model(current_seq)\n",
    "\n",
    "            # Extract the last generated token (the prediction for the next step)\n",
    "            next_token = out[:, :, -1].unsqueeze(2)\n",
    "\n",
    "            # Append new token\n",
    "            total_seq = torch.cat([total_seq, next_token], dim=2)\n",
    "\n",
    "            # Remove first token to keep context window fixed\n",
    "            current_seq = total_seq[:,:,-window_size:]\n",
    "\n",
    "    new_seq = total_seq[:,:,-args.max_gen_len:]\n",
    "\n",
    "    return total_seq, new_seq\n",
    "    \n",
    "args.add_gaussian_noise = 1\n",
    "args.max_gen_len = 100\n",
    "\n",
    "for it, (train_data, _, _, _) in enumerate(train_loader):\n",
    "    start_seq = train_data[:, :-1].unsqueeze(1)\n",
    "    break\n",
    "    \n",
    "total_seq, new_seq = autoregressive_sample(model, start_seq, args.max_gen_len)\n",
    "    \n",
    "traj_tot = torch.matmul(R1i,total_seq.unsqueeze(-1)) # Reverse random mapping\n",
    "X_tot = torch.matmul(Pd,traj_tot) # Map back to lower dim\n",
    "\n",
    "traj_new = torch.matmul(R1i,new_seq.unsqueeze(-1)) # Reverse random mapping\n",
    "X_new = torch.matmul(Pd,traj_new) # Map back to lower dim\n",
    "\n",
    "# Predicted trajectory\n",
    "X_tot_plt = X_tot.squeeze(0)[0].detach().cpu().squeeze().numpy()\n",
    "plt.plot(X_tot_plt.T[0], X_tot_plt.T[1], 'r--', label='Predicted') # Added label\n",
    "\n",
    "X_new_plt = X_new.squeeze(0)[0].detach().cpu().squeeze().numpy()\n",
    "plt.plot(X_new_plt.T[0], X_new_plt.T[1], 'b', label='Predicted') # Added label\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
