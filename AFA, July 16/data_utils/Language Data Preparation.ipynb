{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30dcbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.rc('font', size=20)\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import transformers\n",
    "# import datasets\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# from collections import Counter\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from tqdm import tqdm # Loading bar\n",
    "print('Done.')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4807325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from utils import complex_conj_transpose, batched_complex_conj_transpose, complex_exp, complex_exp_v2, complex_hadamard, complex_matmul, complex_division\n",
    "from utils import batched_complex_conj_transpose, batched_complex_hadamard, batched_complex_matmul, batched_complex_division\n",
    "from utils import batched_complex_exp, batched_complex_hadamard_full, batched_complex_matmul_full\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4627f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from dynamics import stochastic_LTI, DynamicSim\n",
    "from dynamics import construct_mapping\n",
    "from dynamics import get_nth_measurement, get_random_measurements\n",
    "from dynamics import linear_spiral, linear_spiral_3D, Lorenz, rand_coupling_matrix, Van_der_Pol_osc\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdcaf091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from precision_attention import compute_residuals, compute_kernel_v1, compute_estimates_and_residuals_vectorized, get_time_diffs, compute_neg_kernel, clamp_exponent_arg\n",
    "from precision_attention import compute_kernel, batched_compute_estimates_and_residuals_vectorized, compute_estimates_and_residuals_irregular_times, compute_nu\n",
    "from precision_attention import compute_precision_v1\n",
    "# from precision_attention import precise_attn, precise_attn_with_correction, precise_attn_full\n",
    "from precision_attention import compute_precision, compute_precision_tanh\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c19e69bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from model import compute_lambda_h\n",
    "from model import init_complex_matrix, build_nearly_identity, initialize_to_correct_model\n",
    "from model import init_weight_masks, apply_weight_masks\n",
    "from model import Complex_MSE_Loss, Batched_Complex_MSE_Loss, inverse_penalty\n",
    "from model import BatchedPrecisionAttentionBlock\n",
    "from model import HadamardLayer, TemporalNorm, TemporalWhiteningLayer\n",
    "from model import PrecisionNet_1layer, PrecisionNet\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c14d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser('DA')\n",
    "parser.add_argument('--gpu', type=int, default=0) # (Default: 0)\n",
    "args = parser.parse_args(args=[])\n",
    "args.device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
    "print(args.device)\n",
    "\n",
    "torch.manual_seed(2025)\n",
    "np.random.seed(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dfada6",
   "metadata": {},
   "source": [
    "### Load in Wikitext data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acde2d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DataFrame Head (First 5 rows) ---\n",
      "                                                text\n",
      "0                                                   \n",
      "1                              = Robert Boulter = \\n\n",
      "2                                                   \n",
      "3   Robert Boulter is an English film , televisio...\n",
      "4   In 2006 , Boulter starred alongside Whishaw i...\n",
      "\n",
      "--- DataFrame Info (Columns, Non-Null Counts, Dtypes) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44836 entries, 0 to 44835\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    44836 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 350.4+ KB\n",
      "\n",
      "--- DataFrame Description (Statistical Summary) ---\n",
      "         text\n",
      "count   44836\n",
      "unique  26538\n",
      "top          \n",
      "freq    15717\n",
      "\n",
      "--- DataFrame Shape (Rows, Columns) ---\n",
      "Shape: (44836, 1)\n",
      "\n",
      "--- DataFrame Columns ---\n",
      "['text']\n"
     ]
    }
   ],
   "source": [
    "# Visualize data\n",
    "\n",
    "local_data_dir = r\"C:\\Users\\Pracioppo\\Desktop\\Peter DynAttn Proj\\data\\wikitext\"\n",
    "\n",
    "# Replace 'your_file.parquet' with the actual path to your Parquet file\n",
    "\n",
    "try:\n",
    "    # Load the Parquet file into a Pandas DataFrame\n",
    "    df = pd.read_parquet(local_data_dir)\n",
    "\n",
    "    print(\"--- DataFrame Head (First 5 rows) ---\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\n--- DataFrame Info (Columns, Non-Null Counts, Dtypes) ---\")\n",
    "    df.info()\n",
    "\n",
    "    print(\"\\n--- DataFrame Description (Statistical Summary) ---\")\n",
    "    print(df.describe()) # For numerical columns\n",
    "\n",
    "    print(f\"\\n--- DataFrame Shape (Rows, Columns) ---\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "\n",
    "    print(f\"\\n--- DataFrame Columns ---\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae86ea5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load datasets via Pandas workaround...\n",
      "Parquet files loaded into Pandas DataFrames successfully.\n",
      "Pandas DataFrames converted to Hugging Face Dataset objects.\n",
      "\n",
      "Dataset loaded successfully via Pandas workaround!\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# --- Define the local path where you saved the files ---\n",
    "# IMPORTANT: Replace this with the actual path on your computer!\n",
    "local_data_dir = r\"C:\\Users\\Pracioppo\\Desktop\\Peter DynAttn Proj\\data\\wikitext\"\n",
    "\n",
    "# Define the full paths to your parquet files\n",
    "train_file = f\"{local_data_dir}/train-00000-of-00001.parquet\"\n",
    "validation_file = f\"{local_data_dir}/validation-00000-of-00001.parquet\"\n",
    "test_file = f\"{local_data_dir}/test-00000-of-00001.parquet\"\n",
    "\n",
    "try:\n",
    "    print(\"Attempting to load datasets via Pandas workaround...\")\n",
    "\n",
    "    # Load each parquet file into a Pandas DataFrame\n",
    "    train_df = pd.read_parquet(train_file)\n",
    "    validation_df = pd.read_parquet(validation_file)\n",
    "    test_df = pd.read_parquet(test_file)\n",
    "\n",
    "    print(\"Parquet files loaded into Pandas DataFrames successfully.\")\n",
    "\n",
    "    # Convert Pandas DataFrames to Hugging Face Dataset objects\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    validation_dataset = Dataset.from_pandas(validation_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    print(\"Pandas DataFrames converted to Hugging Face Dataset objects.\")\n",
    "\n",
    "    # Create a DatasetDict from these Dataset objects\n",
    "    raw_wiki_dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': validation_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "\n",
    "    print(\"\\nDataset loaded successfully via Pandas workaround!\")\n",
    "    print(raw_wiki_dataset)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: One or more local files not found in '{local_data_dir}'.\")\n",
    "    print(\"Please ensure the 'local_data_dir' path is correct and the files (train-*.parquet, validation-*.parquet, test-*.parquet) are present.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during Pandas loading: {e}\")\n",
    "    print(\"Please ensure you have pandas and pyarrow installed:\")\n",
    "    print(\"pip install pandas pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8254690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(examples):\n",
    "    cleaned_texts = []\n",
    "    allowed_chars_pattern = r\"[a-zA-Z0-9\\s.,!?'\\\"-]\"\n",
    "#     allowed_chars_pattern = r\"[\\x20-\\x7E\\t\\n\\r]\"\n",
    "    for text in examples[\"text\"]:\n",
    "        # Strip leading/trailing whitespace (including newlines)\n",
    "        stripped_text = text.strip()\n",
    "\n",
    "        # Filter out empty strings and Wikipedia-style headings\n",
    "        if not stripped_text or \\\n",
    "           (stripped_text.startswith('=') and stripped_text.endswith('=') and len(stripped_text) < 50) or \\\n",
    "           len(stripped_text) < 50: # Filter very short lines that are likely not useful sentences\n",
    "            continue # Skip this example if it matches criteria\n",
    "\n",
    "        # Remove common Wikitext artifacts like '@-@', '= =' using string.replace()\n",
    "        cleaned_text = stripped_text.replace(\"@\", \" \") # Replace \" @ \" with a space\n",
    "        cleaned_text = cleaned_text.replace(\" = = \", \" \")  # Replace \" = = \" with a space\n",
    "\n",
    "        # Remove non standard chars\n",
    "#         cleaned_text = unicodedata.normalize('NFKC', cleaned_text)\n",
    "        cleaned_text = re.sub(r'[^' + allowed_chars_pattern + ']', '', cleaned_text)\n",
    "        cleaned_text = re.sub(r'\\(.*?\\)', '', cleaned_text)\n",
    "        \n",
    "        # Remove words that are entirely in uppercase\n",
    "        words = cleaned_text.split() # Split the current cleaned_text into words\n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if not (word.strip(string.punctuation).isalpha() and word.strip(string.punctuation).isupper())\n",
    "        ]\n",
    "        cleaned_text = ' '.join(filtered_words)\n",
    "        \n",
    "        # Normalize spaces (replace multiple spaces with a single space)\n",
    "        cleaned_text = ' '.join(cleaned_text.split())\n",
    "        \n",
    "        # Remove spaces before punctuation\n",
    "        cleaned_text = re.sub(r'\\s+(' + r\"[.,!?;:\\\"\\'%]\" + ')', r'\\1', cleaned_text)\n",
    "\n",
    "        if cleaned_text: # Ensure it's not empty after all cleaning steps\n",
    "            cleaned_texts.append(cleaned_text)\n",
    "\n",
    "    return {\"text\": cleaned_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b423a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e547a53ea65494fbece11488ad896bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae03eb446794c1e8f105aca5017af0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce512996e0a49e2b345c8aa3b9363c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki_dataset = raw_wiki_dataset.map(\n",
    "    clean_text,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"], # The 'text' column will be replaced by the cleaned one\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df90a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train split size: 36718\n",
      "Cleaned train split size: 16214\n",
      "Example cleaned text from train split (first 500 chars):\n",
      "6718\n",
      "On July 24, the Pacificus encountered a storm with maximum sustained winds meeting the threshold of tropical storm status roughly 1, 000 mi west of the Baja California Peninsula. Despite reports that the location of the system remained vague, the Joint Typhoon Warning Center began issuing tropical cyclone advisories and warnings on the unnamed disturbance. Tracking west - northwestward, the tropical storm peaked with winds reported at 65 mph shortly after its discovery; however, the discontinued\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "618"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Original train split size: {len(raw_wiki_dataset['train'])}\")\n",
    "print(f\"Cleaned train split size: {len(wiki_dataset['train'])}\")\n",
    "print(f\"Example cleaned text from train split (first 500 chars):\")\n",
    "# print(cleaned_dataset['train'][0]['text'][:500])\n",
    "\n",
    "i = np.random.choice(len(wiki_dataset['train']))\n",
    "print(i)\n",
    "print(wiki_dataset['train'][i]['text'][:500])\n",
    "\n",
    "len(wiki_dataset['train'][i]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f76dc6d",
   "metadata": {},
   "source": [
    "### Alternatively, load in from txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4e7e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_dir = r\"C:\\Users\\Pracioppo\\Desktop\\Peter DynAttn Proj\\data\\poets\"\n",
    "\n",
    "names = [\"shakespeare\", \"shelley\", \"milton\"]\n",
    "all_cleaned_text = []\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    file_path = txt_dir + \"\\\\\" + name + \".txt\"\n",
    "\n",
    "    # 1. Load the text file\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            full_text = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "\n",
    "    data_for_cleaning = {\"text\": [full_text]}\n",
    "\n",
    "    cleaned_result = clean_text(data_for_cleaning)\n",
    "\n",
    "    all_cleaned_text.extend(cleaned_result[\"text\"])\n",
    "    \n",
    "combined_text = \" \".join(all_cleaned_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87db35c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segments: 22863\n"
     ]
    }
   ],
   "source": [
    "def split_segments_by_nth_period(text, n=3):\n",
    "    \"\"\"\n",
    "    Splits a given text into segments, where each segment ends after n periods.\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    current_start_index = 0\n",
    "    period_count = 0\n",
    "\n",
    "    for i, char in enumerate(text):\n",
    "        if char == '.':\n",
    "            period_count += 1\n",
    "        \n",
    "        # If we've hit 'n' periods, or if we're at the very end of the text\n",
    "        if period_count == n or (i == len(text) - 1 and current_start_index < len(text)):\n",
    "            segment = text[current_start_index : i + 1].strip()\n",
    "            if segment: # Only add non-empty segments\n",
    "                segments.append(segment)\n",
    "            current_start_index = i + 1\n",
    "            period_count = 0 # Reset count for the next segment\n",
    "    \n",
    "    # Handle any remaining text if the last segment didn't end with 'n' periods\n",
    "    # and wasn't caught by the (i == len(text) - 1) condition\n",
    "    if current_start_index < len(text):\n",
    "        remaining_text = text[current_start_index:].strip()\n",
    "        if remaining_text:\n",
    "            segments.append(remaining_text)\n",
    "            \n",
    "    return segments\n",
    "\n",
    "segments = split_segments_by_nth_period(combined_text)\n",
    "print(f\"Number of segments: {len(segments)}\")\n",
    "\n",
    "random.shuffle(segments) # This shuffles the list in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f51fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_split_dataset = Dataset.from_dict({\"text\": segments})\n",
    "\n",
    "train_test_split = pre_split_dataset.train_test_split(test_size=0.2, seed=42) # Using a seed for reproducibility\n",
    "\n",
    "# Access the training and initial test sets\n",
    "train_dataset = train_test_split['train']\n",
    "test_val_dataset = train_test_split['test'] # This will be split further into test and validation\n",
    "\n",
    "# Now, split the test_val_dataset into actual test and validation sets\n",
    "# We'll split the remaining 20% in half, so 10% for test and 10% for validation of the overall data\n",
    "test_val_split = test_val_dataset.train_test_split(test_size=0.5, seed=42) \n",
    "\n",
    "test_dataset = test_val_split['train'] # This becomes the true test set\n",
    "val_dataset = test_val_split['test']   # This becomes the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76dc9a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "poets_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset, # Use 'validation' as the standard key for validation set\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9116985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIKI DATASET:\n",
      "  train: 16214 examples\n",
      "  validation: 1734 examples\n",
      "  test: 1945 examples\n",
      "  train: 10035232 characters\n",
      "  validation: 1057589 characters\n",
      "  test: 1180421 characters\n",
      "==================\n",
      "POETS DATASET:\n",
      "  train: 18290 examples\n",
      "  validation: 2287 examples\n",
      "  test: 2286 examples\n",
      "  train: 5858316 characters\n",
      "  validation: 724226 characters\n",
      "  test: 715568 characters\n"
     ]
    }
   ],
   "source": [
    "print('WIKI DATASET:')\n",
    "for split_name, dataset_obj in wiki_dataset.items():\n",
    "    print(f\"  {split_name}: {len(dataset_obj)} examples\")\n",
    "\n",
    "for split_name, dataset_obj in wiki_dataset.items():\n",
    "    total_chars = sum(len(example['text']) for example in dataset_obj)\n",
    "    print(f\"  {split_name}: {total_chars} characters\")\n",
    "\n",
    "print('==================')\n",
    "\n",
    "print('POETS DATASET:')\n",
    "for split_name, dataset_obj in poets_dataset.items():\n",
    "    print(f\"  {split_name}: {len(dataset_obj)} examples\")\n",
    "\n",
    "for split_name, dataset_obj in poets_dataset.items():\n",
    "    total_chars = sum(len(example['text']) for example in dataset_obj)\n",
    "    print(f\"  {split_name}: {total_chars} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7533c15",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12921197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# local_bert_dir = r\"C:\\Users\\Pracioppo\\Desktop\\Peter DynAttn Proj\\data\\local_bert_dir\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(local_bert_dir)\n",
    "# print(\"Tokenizer loaded!\")\n",
    "\n",
    "local_bert_mini_dir = r\"C:\\Users\\Pracioppo\\Desktop\\Peter DynAttn Proj\\data\\prajjwal1_bert_mini\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_bert_mini_dir)\n",
    "print(\"Tokenizer loaded!\")\n",
    "\n",
    "# Ensure tokenizer has a pad_token if your model or DataCollator needs it\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    print(\"Added padding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6119fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # This function uses the 'tokenizer' object defined globally above\n",
    "    return tokenizer(examples[\"text\"]) # No truncation/max_length here for LM concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1f66341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0cfdd019a44b67a53bb096279cd791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f9e1db58114c56b7836ce1de6f520f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae0003c5e9b473cb6a4e5c2d45c6ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply Tokenization\n",
    "print(\"\\nTokenizing dataset...\")\n",
    "tokenized_wiki_dataset = wiki_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"], # This removes the original 'text' column after tokenization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32f2b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Grouping Function for Language Modeling\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af49f085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grouping texts into blocks of size 128 for language modeling...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43b4a0d716c4298ab5eb0edddeec056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84a336c7aba4d9f987d6357ecf5b2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfad404a52b4a9ca9094bb6ae38775f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts grouped for language modeling.\n"
     ]
    }
   ],
   "source": [
    "# Define Block Size for Language Modeling\n",
    "block_size = 128\n",
    "\n",
    "# Apply Grouping\n",
    "print(f\"\\nGrouping texts into blocks of size {block_size} for language modeling...\")\n",
    "lm_dataset = tokenized_wiki_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True\n",
    ")\n",
    "lm_dataset.set_format(\"torch\")\n",
    "print(\"Texts grouped for language modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c73d9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Tokenization\n",
    "print(\"\\nTokenizing dataset...\")\n",
    "tokenized_poets_dataset = poets_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"], # This removes the original 'text' column after tokenization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbcf048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grouping texts into blocks of size 128 for language modeling...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b575b1ec5e4f0697374c8471df29e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab49138fc1a464d9af1eb2ee2e26ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9986cb76188417cac0f47ba66583c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts grouped for language modeling.\n"
     ]
    }
   ],
   "source": [
    "# Define Block Size for Language Modeling\n",
    "block_size = 128\n",
    "\n",
    "# Apply Grouping\n",
    "print(f\"\\nGrouping texts into blocks of size {block_size} for language modeling...\")\n",
    "lm_dataset = tokenized_poets_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True\n",
    ")\n",
    "lm_dataset.set_format(\"torch\")\n",
    "print(\"Texts grouped for language modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6530b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_similarity(query_emb, vocab_embs, alpha = 1):\n",
    "#     \"\"\"\n",
    "#     Calculates custom similarity\n",
    "#     \"\"\"\n",
    "\n",
    "#     diff = query_emb.unsqueeze(0) - vocab_embs\n",
    "#     p = 1/(alpha + diff**2)\n",
    "    \n",
    "#     scores = torch.mean(p,axis=1)\n",
    "    \n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b04bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_to_nearest_token(vocab_embed, query_embed, similar_fn=1):\n",
    "    \"\"\"\n",
    "    Given an embedding, find the nearest embedding in the vocabulary and output the corresponding token.\n",
    "    \"\"\"\n",
    "\n",
    "    if similar_fn == 1:    \n",
    "        diff = query_emb.unsqueeze(0) - vocab_embs\n",
    "        p = 1/(alpha + diff**2)\n",
    "        similarities = torch.mean(p,axis=1)\n",
    "    else:\n",
    "        similarities = F.cosine_similarity(query_embed.unsqueeze(0), vocab_embeddings, dim=1)\n",
    "\n",
    "    # Find the top K similarities and their indices\n",
    "    top_k_similarities, top_k_indices = torch.topk(similarities, k=1)\n",
    "\n",
    "    # Get the token ID\n",
    "    token_id = top_k_indices.item()\n",
    "    # Convert the token ID back to a readable token string\n",
    "    token_string = tokenizer.convert_ids_to_tokens(token_id)\n",
    "    similarity_score = top_k_similarities.item()\n",
    "\n",
    "    return token_string, similarity_score\n",
    "\n",
    "# query_embed = initial_input_embeddings[0,0]\n",
    "# vocab_embed = model.embeddings.word_embeddings.weight\n",
    "\n",
    "# token_string, similarity_score = embedding_to_nearest_token(vocab_embed, query_embed)\n",
    "\n",
    "# print(f\"Token: '{token_string}', Similarity: {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a47da2",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41129637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, pad_token_id=0, layer_norm_eps=1e-12, hidden_dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        # Word Embeddings: Maps token IDs to vectors\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n",
    "        \n",
    "        # Positional Embeddings: Maps token positions to vectors\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        \n",
    "        # Token Type Embeddings: Maps segment IDs (e.g., 0 for first sentence, 1 for second) to vectors\n",
    "        self.token_type_embeddings = nn.Embedding(type_vocab_size, hidden_size)\n",
    "\n",
    "        # Layer Normalization: Normalizes the sum of embeddings\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
    "        \n",
    "        # Dropout: Regularization to prevent overfitting\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "        # Register position_ids buffer (used if position_ids are not provided in forward)\n",
    "        self.register_buffer(\"position_ids\", torch.arange(max_position_embeddings).expand((1, -1)))\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        input_shape = input_ids.size()\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        # 1. Get Word Embeddings\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "\n",
    "        # 2. Get Token Type Embeddings (if not provided, default to all 0s)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=input_ids.device)\n",
    "        token_type_embeds = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        # 3. Get Positional Embeddings (if not provided, generate default sequence)\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length] # Use the pre-registered buffer\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "\n",
    "        # 4. Sum them up\n",
    "        embeddings = inputs_embeds + token_type_embeds + position_embeds\n",
    "        \n",
    "        # 5. Apply Layer Normalization and Dropout\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
