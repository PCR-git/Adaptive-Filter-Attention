{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de78b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchedPrecisionAttentionBlock(nn.Module):\n",
    "  def __init__(self, args):\n",
    "    \"\"\"\n",
    "    Initializes the batched precision-weighted attention block.\n",
    "\n",
    "    Parameters:\n",
    "        W_q, W_k, W_v, W_o (torch.Tensor): Learnable weight matrices (query, key, value, and output).\n",
    "        nu (float): Scaling parameter.\n",
    "        args: Additional model/system parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    super().__init__()\n",
    "\n",
    "    assert args.d_e % 2 == 0 # d_e must be divisible by 2, since eigenvalues come in complex conjugate pairs\n",
    "\n",
    "    #####################\n",
    "\n",
    "    sqrt_dv = torch.sqrt(torch.tensor(args.d_v))\n",
    "  \n",
    "    self.W_q = nn.Parameter(init_complex_matrix(args.d_k, args.d_e)) # Query weight matrix\n",
    "    self.W_k = nn.Parameter(init_complex_matrix(args.d_k, args.d_e)) # Key weight matrix\n",
    "    self.W_v = nn.Parameter(init_complex_matrix(args.d_v, args.d_e)) # Value weight matrix\n",
    "    self.W_r = nn.Parameter(init_complex_matrix(args.d_v, args.d_e)) # Residual weight matrix\n",
    "    self.W_p = nn.Parameter(init_complex_matrix(args.d_e, args.d_v)) # Prediction output weight matrix\n",
    "    \n",
    "    #####################\n",
    "\n",
    "    self.args = args\n",
    "    self.nu = 1 # Measurement weighting in attention; Just set to 1 for now, since this can be absorbed into weight matrices\n",
    "\n",
    "    self.causal_mask = torch.tril(torch.ones(args.Npts, args.Npts)).view(1, args.Npts, args.Npts, 1, 1).to(args.device) # Causal attention mask\n",
    "\n",
    "  def forward(self, X, t_measure_all):\n",
    "    \"\"\"\n",
    "    Forward pass through the precision-weighted attention block.\n",
    "\n",
    "    Parameters:\n",
    "        X (torch.Tensor): Input data.\n",
    "        lambder_h (torch.Tensor): Diagonal of state transition matrix.\n",
    "        lambda_Omega (torch.Tensor): Process noise covariance.\n",
    "        lambda_Omega0 (torch.Tensor): Initial process noise covariance.\n",
    "        lambda_C (torch.Tensor): Measurement output matrix.\n",
    "        lambda_Gamma (torch.Tensor): Measurement noise covariance.\n",
    "        t_measure_all (torch.Tensor): Time differences vector, for each trajectory in batch.\n",
    "\n",
    "    Returns:\n",
    "        out (torch.Tensor): Output tensor.\n",
    "        Q_ij (torch.Tensor): Normalized attention weights.\n",
    "        X_ij_hat_all (torch.Tensor): Estimated values.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_q = torch.matmul(W_q, X)\n",
    "    X_k = torch.matmul(W_k, X)\n",
    "    X_v = torch.matmul(W_v, X)\n",
    "\n",
    "    # Compute unnormalized attention matrix\n",
    "    mahalanobis_distance = R_qk_ij**2\n",
    "    denom = (1 + nu*torch.sum(mahalanobis_distance, axis=3, keepdims = True))\n",
    "    A_ij = 1 / denom\n",
    " \n",
    "    A_ij = A_ij * self.causal_mask # Apply causal mask to attention matrix\n",
    "    X_ij_hat_all = X_ij_hat_all * self.causal_mask # Mask out estimates backward in time (not strictly necessary but useful larter for visualization)\n",
    "    \n",
    "    # Normalize attention\n",
    "    S_ij = torch.sum(A_ij, axis=2, keepdims = True)\n",
    "    Q_ij = A_ij / S_ij\n",
    "    \n",
    "    # Compute Hadamard product and sum to get estimate in diagonalized space\n",
    "#     est_v = torch.sum(Q_ij * X_ij_hat_all,axis=3)\n",
    "    est_v = torch.sum(Q_ij.unsqueeze(1) * X_v)\n",
    "\n",
    "    # Add residual connection\n",
    "    est_latent = est_v # No residual connection\n",
    "    \n",
    "    # Multiply by output matrix to get estimate\n",
    "    out = batched_complex_matmul(W_p,est_latent)\n",
    "     \n",
    "    return out, Q_ij"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
