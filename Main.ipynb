{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55sKZVe2NcuJ"
   },
   "source": [
    "## Adaptive Filter Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1IgNYUyROjm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.rc('font', size=20)\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import transformers\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm # Loading bar\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import complex_matmul, apply_interleaved_rope\n",
    "from utils import count_parameters, get_layers\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamics import stochastic_LTI, DynamicSim\n",
    "from dynamics import construct_mapping\n",
    "from dynamics import get_nth_measurement, get_random_measurements\n",
    "from dynamics import linear_spiral, linear_spiral_3D, Lorenz, rand_coupling_matrix, Van_der_Pol_osc\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isotropic_afa import get_safe_exp_tot, compute_covariance_matrix, compute_covariance_matrix_safe\n",
    "from isotropic_afa import compute_exp_kernel_isotropic, compute_residual_norm_isotropic\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import compute_lambda, resolve_multihead_dims\n",
    "from model import init_complexlinear, init_complex_matrix, set_complex_weight, initialize_linear_layers\n",
    "from model import initialize_to_correct_model\n",
    "from model import apply_weight_masks\n",
    "from model import ComplexLinearLayer, ComplexLinearHermitianLayer, ComplextoRealLinearLayer\n",
    "from model import MultiHeadAttentionLayer\n",
    "from model import ComplexRMSNorm\n",
    "from model import Attention_1layer, AFA_1layer\n",
    "from model import SimpleAttention_Net\n",
    "from model import TransformerBlock, TransformerNetwork\n",
    "from model import MultiheadIsotropicAFA_1layer, AFATransformerBlock, AFATransformerNetwork\n",
    "# from model import compute_attention_matrix, compute_estimate\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import construct_random_mapping, construct_data_dynamics, TrainDataset_dynamics, create_train_loader_dynamics\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import plot_trajectory, compute_state_matrix, plot_state_matrix, plot_eigenvals, visualize_results\n",
    "from visualization import visualize_results_attn, _get_visual_modules\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import single_iter, single_epoch, hook_fn\n",
    "from training import single_iter_attn, single_epoch_attn\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeWE9GM-R_40"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser('DA')\n",
    "parser.add_argument('--gpu', type=int, default=0) # (Default: 0)\n",
    "args = parser.parse_args(args=[])\n",
    "args.device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
    "print(args.device)\n",
    "\n",
    "torch.manual_seed(2025)\n",
    "np.random.seed(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dynamical system\n",
    "\n",
    "# # Normal system\n",
    "# D1 = torch.zeros(2,2,2).to(args.device) # Diagonal matrix\n",
    "# D1[0] = torch.tensor([[-0.1, 0.0], [0.0, -0.1]]).to(args.device)\n",
    "# D1[1] = torch.tensor([[-1.0, 0.0], [0.0, 1.0]]).to(args.device)\n",
    "# # S1 = torch.zeros(2,2,2).to(args.device)\n",
    "# # S1[0] = torch.tensor(([1.0,1.0],[0.0,0.0]))\n",
    "# # S1[1] = torch.tensor(([0.0,0.0],[1.0,-1.0]))\n",
    "# # S1 = U/np.sqrt(2)\n",
    "# alpha = np.random.uniform(low=0.0, high=1.0)*2*np.pi\n",
    "# beta = np.random.uniform(low=0.0, high=1.0)*2*np.pi\n",
    "# S1 = construct_special_2D_unitary(alpha=alpha, beta=beta)\n",
    "# Si1 = complex_conj_transpose(S1)\n",
    "\n",
    "# Stable 2D linear system (in diagonalized form):\n",
    "D1 = torch.zeros(2,2,2).to(args.device) # Diagonal matrix\n",
    "S1 = torch.zeros(2,2,2).to(args.device) # RHS matrix\n",
    "Si1 = torch.zeros(2,2,2).to(args.device) # Inverse of RHS matrix\n",
    "D1[0] = torch.tensor([[-0.1, 0.0], [0.0, -0.1]]).to(args.device) # Negative real part eigenvals\n",
    "# D1[0] = torch.tensor([[0.0, 0.0], [0.0, 0.0]]).to(args.device) # Zero real part eigenvals\n",
    "# D1[0] = torch.tensor([[0.1, 0.0], [0.0, 0.1]]).to(args.device) # Positive real part eigenvals\n",
    "D1[1] = torch.tensor([[-1.0, 0.0], [0.0, 1.0]]).to(args.device)\n",
    "S1[0] = torch.tensor([[1.0, 1.0], [1.0, 1.0]]).to(args.device)\n",
    "S1[1] = torch.tensor([[-1.0, 1.0], [0.0, 0.0]]).to(args.device)\n",
    "Si1[0] = 0.5*torch.tensor([[0.0, 1.0], [0.0, 1.0]]).to(args.device)\n",
    "Si1[1] = 0.5*torch.tensor([[1.0, -1.0], [-1.0, 1.0]]).to(args.device)\n",
    "A = complex_matmul(S1,complex_matmul(D1,Si1))[0].unsqueeze(0)\n",
    "params = [D1, S1, Si1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7F6aLjrv8-gS",
    "outputId": "fbf50e25-b410-4406-a22f-4543fe725e59"
   },
   "outputs": [],
   "source": [
    "# DEFINE MODEL\n",
    "\n",
    "args.cr_max = 2\n",
    "args.cr_min = 0\n",
    "\n",
    "args.max_exponent = 10\n",
    "args.min_exponent = -10\n",
    "\n",
    "args.t_equal = 1 # Equal time intervals? (0 or 1)\n",
    "args.sep_params = 0 # Use separate params for keys and values? (0 or 1)\n",
    "\n",
    "##################################################\n",
    "## Testing options\n",
    "args.rand_embed = 0 # Use random orthogonal matrix when projecting data to higher dimension?\n",
    "args.weight_mask = 0 # Mask the weights? (0 or 1) (must be set to 1 when visualizing in 2D)\n",
    "args.lambda_real_zero = 0 # Force real part of lambda to be zero? (0=No, 1=Yes)\n",
    "args.compute_pulled_forward_estimates = 1\n",
    "##################################################\n",
    "\n",
    "##################################################\n",
    "## Ablation options ##\n",
    "######################\n",
    "## (Setting everything to 0 (except lambda_real_zero & outer residual norm) makes it an ordinary Transformer)\n",
    "args.lambda_real_zero = 0 # Zero out real part of eigenvalues? (1 = yes, 0 = no)\n",
    "args.use_full_residual_norm = 1 # Use the full |R|^2 metric and rational robust weight (1) or dot product / exponential weight (0)?\n",
    "args.use_robust_weight = 1 # Use rational weight rather than softmax\n",
    "args.additive_bias_type = 1 # (Additive bias: 0 for zero; 1 for DLE; 2 for linear)\n",
    "args.multiplicative_bias_type = 1 # (Multiplicative bias: 0 for constant; 1 for DLE; 2 for linear)\n",
    "args.compute_next_step_pred = 1 # Compute next-step prediction? (1 = yes, 0 = No)\n",
    "args.learn_rotations = 1 # Learned rotations (1), or fixed as in RoPE (0)?\n",
    "args.rotate_values = 1 # Rotate/unrotate values? (1 = yes, 0 = No)\n",
    "args.zero_process_noise = 0 # Zero process noise (sigma^2)? (1 = yes, 0 = No)\n",
    "args.zero_key_measurement_noise = 0 # Zero key measurement noise (eta^2)? (1 = yes, 0 = No)\n",
    "args.use_total_precision_gate = 2 # Use total-precision gating? (0 = No gate, 1 = precision gate, 2 = learned gate)\n",
    "args.use_complex_input_norm = 0 # Use complex-valued RMS Norm AFTER input projection (1) or real-valued RMS Norm BEFORE input projection (0)?\n",
    "args.use_complex_output_norm = 0 # Use complex-valued RMS Norm BEFORE output projection (1) or real-valued RMS Norm AFTER output projection (0)?\n",
    "args.use_inner_residual = 1 # Include a residual connection BEFORE output projection (1) or AFTER output projection (0)?\n",
    "args.add_gaussian_noise = 0 # Add Gaussian noise to final token (for test-time sampling)\n",
    "\n",
    "if args.use_inner_residual == 0 and (args.use_total_precision_gate == 1 or args.use_total_precision_gate == 2):\n",
    "    print('Warning: inner residual is off.')\n",
    "\n",
    "##################################################\n",
    "\n",
    "args.m = 2 # Dimension of simulated system\n",
    "# args.embed_dim = 2 # Embedding dimension\n",
    "args.d_e = 128 # Embedding dimension\n",
    "\n",
    "# args.num_heads = 1\n",
    "# args.head_dim = int(args.embed_dim/args.num_heads)\n",
    "\n",
    "##################################################\n",
    "\n",
    "# Key, query, value embedding dimensions are same as input embedding dimension\n",
    "args.d_k = args.d_e\n",
    "args.d_v = args.d_e\n",
    "# args.d_k = args.head_dim\n",
    "# args.d_v = args.head_dim\n",
    "\n",
    "# # Key, query, value embedding dimensions are half of input embedding dimension\n",
    "# args.d_k = int(args.head_dim/2) # Key and query embedding dimension\n",
    "# args.d_v = int(args.head_dim/2) # Value embedding dimension\n",
    "\n",
    "# # Key, query, value embedding dimensions are 2\n",
    "# args.d_k = 2 # Key and query embedding dimension\n",
    "# args.d_v = 2 # Value embedding dimension\n",
    "\n",
    "##################################################\n",
    "\n",
    "args.nu = 1.0 # Measurement weighting\n",
    "args.tf = 10.0 # Final time\n",
    "args.dt = 0.01 # Time step size\n",
    "args.n = 10 # nth measurement (ie use every nth point as a measurement)\n",
    "args.delta_t = args.dt * args.n\n",
    "\n",
    "args.N_t = int(args.tf/args.dt) # Number of time steps\n",
    "args.seq_len = int(args.N_t/args.n)+1 # Number of measurements\n",
    "# args.seq_len = args.N_t / args.n = (args.tf/args.dt) / (args.delta_t/args.dt) = args.tf/args.delta_t\n",
    "t_v = (torch.arange(args.N_t + args.n + 1)*args.dt).to(args.device) # Array of time steps\n",
    "\n",
    "# Some scalar weights in model\n",
    "args.alpha = 1.0\n",
    "args.beta = 0.0\n",
    "args.delta = 1.0\n",
    "args.eta = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE MODEL:\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# # Standard Attention/Transformer Models\n",
    "\n",
    "# args.model_type = 'RealInputs'\n",
    "# args.num_heads = 4\n",
    "# args.num_blocks = 3\n",
    "# # model = Attention_1layer(args.d_e, args.d_v, args.num_heads, args).to(args.device)\n",
    "# # model = SimpleAttention_Net(args.d_e, args.d_v*2, args.num_heads, args).to(args.device)\n",
    "# model = TransformerNetwork(args.d_e, args.d_v*2, args.d_v*4, args.num_heads, args, num_blocks=args.num_blocks).to(args.device)\n",
    "# loss = nn.MSELoss()\n",
    "# loss_p = nn.MSELoss()\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# # Complex Attention with Real Input/Output Models\n",
    "\n",
    "# args.model_type = 'RealInputs'\n",
    "# args.metric = 'RealDotProduct'\n",
    "# # args.metric_type = 'InverseMahalanobis'\n",
    "# # model = SimpleComplexAttention_Net(args.d_e, args.d_v, args).to(args.device)\n",
    "# model = ComplexTransformerNetwork(args.d_e, args.d_v*2, args.d_v*4, args, num_blocks=2).to(args.device)\n",
    "# loss = nn.MSELoss()\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# Multihead Isotropic AFA\n",
    "\n",
    "args.num_heads = 2 # Number of heads\n",
    "args.compute_next_step_pred = 1\n",
    "args.use_inner_residual = 1\n",
    "# args.d_k = args.d_e\n",
    "# args.d_v = args.d_e\n",
    "# model = MultiheadSimplifiedAFA_1layer(args, args.seq_len, args.num_heads, input_dim=args.d_e, query_key_dim=args.d_k, value_dim=args.d_v).to(args.device)\n",
    "args.d_k_total = args.d_e # Total query-key dim across all heads\n",
    "args.d_v_total = args.d_e # Total value dim across all heads\n",
    "model = MultiheadIsotropicAFA_1layer(args, args.num_heads, input_dim=args.d_e, query_key_dim_total=args.d_k_total, value_dim_total=args.d_v_total).to(args.device)\n",
    "\n",
    "# args.num_blocks = 1 # Number of transformer blocks\n",
    "# model = MultiheadSimplifiedAFATransformerNetwork(args, args.seq_len, num_blocks=args.num_blocks, n_heads=args.num_heads, input_dim=args.d_e, query_key_dim_total=args.d_k_total, value_dim_total=args.d_v_total, Norm=Norm).to(args.device)\n",
    "\n",
    "loss = nn.MSELoss() # Loss\n",
    "loss_p = nn.MSELoss() # Penalty\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# # AFA Transformer\n",
    "\n",
    "# args.compute_next_step_pred = 0\n",
    "# args.num_heads = 2 # Number of heads\n",
    "# args.num_blocks = 2 # Number of transformer blocks\n",
    "# args.d_k_total = args.d_e # Total query-key dim across all heads\n",
    "# args.d_v_total = args.d_e # Total value dim across all heads\n",
    "# model = AFATransformerNetwork(args=args, num_blocks=args.num_blocks, n_heads=args.num_heads, input_dim=args.d_e, query_key_dim_total=args.d_k, value_dim_total=args.d_v, hidden_dim = 4*args.d_v, Norm=nn.LayerNorm).to(args.device)\n",
    "\n",
    "# loss = nn.MSELoss()\n",
    "# loss_p = nn.MSELoss()\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = list(model.parameters()) # Parameters list\n",
    "\n",
    "# loss_p = Complex_Trace_Loss() # Trace Penalty\n",
    "lr = 1E-2 # Learning rate\n",
    "optimizer = torch.optim.Adam(params_list, lr=lr, betas=(0.9, 0.999)) # Optimizer\n",
    "\n",
    "if args.d_k != args.d_v and args.sep_params == 0:\n",
    "    print('ERROR: Key and value embedding dimensions must be the same if using shared parameters.')\n",
    "\n",
    "print('Total parameter count:', count_parameters(model))\n",
    "\n",
    "#####################\n",
    "\n",
    "# Create folders for model weights, and loss history\n",
    "\n",
    "saved_models_path = 'C:/Users/Pracioppo/Desktop/AFA, Dec 11/saved_models/'\n",
    "model_name = str(model.__class__.__name__)\n",
    "date = str(datetime.datetime.today()).split()[0]\n",
    "model_path = saved_models_path + model_name + '__' + date + '/'\n",
    "\n",
    "model_weight_path = model_path + 'model_weights/'\n",
    "model_tensor_path = model_path + 'info_tensors/'\n",
    "\n",
    "try:\n",
    "    os.mkdir(model_path)\n",
    "    os.mkdir(model_weight_path)\n",
    "    os.mkdir(model_tensor_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aJOYmJ79Wmi6",
    "outputId": "d306dcec-02cd-4168-b222-0f1b3bbbc1f7"
   },
   "outputs": [],
   "source": [
    "## CREATE TRAINING DATA\n",
    "\n",
    "args.concat_mag = 0\n",
    "\n",
    "# Training params\n",
    "args.num_epochs = 1000 # Number of epochs\n",
    "args.num_samp = 32 # Number of samples in train loader\n",
    "args.batch_size = 32 # Batch size\n",
    "args.num_its = int(args.num_samp/args.batch_size) # Number of iterations in an epoch\n",
    "args.save_epochs = 100 # Intervals of epochs to save model\n",
    "args.show_example_epochs = 5 # Number of epochs between displaying results so far\n",
    "args.n_example = 5 # Plot state estimates at n_example data points\n",
    "args.epsilon = 1E-5\n",
    "\n",
    "# Process and measurement noise:\n",
    "sigma_process = 0.0 # Process noise\n",
    "sigma_measure = 2.0 # Measurement noise\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# Initialize arrays to record losses\n",
    "mean_epoch_losses = np.zeros(args.num_epochs)\n",
    "log_mean_epoch_losses = np.zeros(args.num_epochs)\n",
    "all_losses = np.zeros(args.num_epochs * args.num_samp)\n",
    "all_lambdas = np.zeros((args.num_epochs, 2, args.d_v))\n",
    "\n",
    "Pu, Pd, R1, R1i = construct_random_mapping(S1, Si1, args) # Get random matrices\n",
    "\n",
    "# Build training dataset\n",
    "train_loader, train_dataset, X_true_all, X_measure_all, t_measure_all = create_train_loader_dynamics(A, S1, Si1, Pu, Pd, R1, R1i, t_v, sigma_process, sigma_measure, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT ALL TRAJECTORIES\n",
    "\n",
    "from matplotlib import patches\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for it, (train_data, X_true, X_measure, t_measure_full) in enumerate(train_loader):\n",
    "# for it, train_data in enumerate(train_loader):\n",
    "#     for i in range(args.batch_size):\n",
    "\n",
    "#     if X_true.size()[2] == 1:\n",
    "#         X_true = torch.stack((X_true, torch.zeros_like(X_true)),dim=2)\n",
    "#         X_measure = torch.stack((X_measure, torch.zeros_like(X_measure)),dim=2)\n",
    "\n",
    "    # Actual trajectory\n",
    "    X_true_plt = X_true.squeeze().detach().cpu().numpy()\n",
    "    plt.plot(X_true_plt.T[0],X_true_plt.T[1],'black')\n",
    "\n",
    "    # Noisy trajectory\n",
    "    traj = X_measure.detach().cpu().squeeze().numpy()\n",
    "    plt.plot(traj.T[0], traj.T[1], 'b--')\n",
    "    \n",
    "    x0 = X_true_plt[:,0].T\n",
    "    plt.scatter(x0[0],x0[1], color='red')\n",
    "\n",
    "#     circle = patches.Circle((0,0), 17.0, edgecolor='red', facecolor='none', linewidth = 2)\n",
    "#     ax.add_patch(circle)\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### Initialize model to correct values (for testing) ######\n",
    "# ##############################################################\n",
    "\n",
    "# if isinstance(model, MultiheadIsotropicAFA_1layer):\n",
    "#     module = model.layers[0]\n",
    "#     initialize_to_correct_model(module, D1, S1, Si1, sigma_process, sigma_measure, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  Load in model weights ######\n",
    "####################################\n",
    "\n",
    "# epoch = 410\n",
    "# model_weight_path_full = model_weight_path + 'weights_epoch_' + str(epoch)\n",
    "\n",
    "# try:\n",
    "#     model.load_state_dict(torch.load(model_weight_path_full, weights_only=True))\n",
    "#     print('Model loaded.')\n",
    "# except:\n",
    "#     print('Model loading failed.')\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### TRAINING LOOP ###########################\n",
    "#####################################################################\n",
    "\n",
    "###### LOOP FOR PRECISION ATTENTION LAYERS #######\n",
    "\n",
    "# Train for num_epochs:\n",
    "for epoch in tqdm(np.arange(args.num_epochs), desc=\"Training progress...\"):\n",
    "\n",
    "    # Train for single epoch\n",
    "    epoch_losses, A_hat_complex, epoch_lambdas, unnormalized_attention = single_epoch(model, train_loader, optimizer, loss, loss_p, params_list, args)\n",
    "\n",
    "    # Collect losses\n",
    "    all_losses[epoch*args.num_its:(epoch+1)*args.num_its] = epoch_losses\n",
    "    mean_epoch_losses[epoch] = np.mean(epoch_losses)\n",
    "    log_mean_epoch_losses[epoch] = np.log(np.mean(epoch_losses))\n",
    "    all_lambdas[epoch,:,:] = np.mean(epoch_lambdas,axis=0)\n",
    "    \n",
    "    # Visualize results so far:\n",
    "    if np.mod(epoch+1,args.show_example_epochs) == 0:\n",
    "        visualize_results(model, train_dataset, all_losses, mean_epoch_losses, log_mean_epoch_losses, all_lambdas, R1, R1i, Pu, Pd, A, epoch, args)\n",
    "\n",
    "#     if np.mod(epoch+1,args.save_epochs) == 0:\n",
    "#         model_weight_path_full = model_weight_path + 'weights_epoch_' + str(epoch)\n",
    "#         model_tensor_path_full = model_tensor_path + 'tensors_epoch_' + str(epoch)\n",
    "#         torch.save(model.state_dict(), model_weight_path_full)\n",
    "\n",
    "#         tensors_to_save = {\n",
    "#         'epoch_losses': epoch_losses,\n",
    "#         'A_hat_complex': A_hat_complex,\n",
    "#         'epoch_lambdas': epoch_lambdas\n",
    "#         }\n",
    "#         torch.save(tensors_to_save, model_tensor_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## JUMP\n",
    "\n",
    "self = model.layers[0]\n",
    "Z_q = Z_k = Z_v = torch.rand([32, 1, 101, 128]).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = Z_q.size()[0]\n",
    "seq_len = Z_q.size()[2]\n",
    "\n",
    "if t_measure_all == None:\n",
    "#             t_measure = torch.arange(seq_len).to(self.args.device) * self.args.delta_t\n",
    "    t_measure = torch.arange(seq_len).to(self.args.device)\n",
    "else:\n",
    "    if len(t_measure_all.size()) > 1:\n",
    "        t_measure = t_measure_all[0,:-1]\n",
    "    else:\n",
    "        t_measure = t_measure_all[:,:-1]\n",
    "\n",
    "# Normalize time vector by total time elapsed (optional)\n",
    "#         t_measure = t_measure / (t_measure[-1] - t_measure[0]).unsqueeze(0)\n",
    "\n",
    "#######################################\n",
    "\n",
    "# Apply linear projections\n",
    "Q_proj = self.W_q(Z_q)\n",
    "K_proj = self.W_k(Z_k)\n",
    "V_proj = self.W_v(Z_v)\n",
    "\n",
    "# Split into real/imaginary parts\n",
    "Q_proj = Q_proj.view(batch_size, seq_len, 2, self.d_k_total)\n",
    "K_proj = K_proj.view(batch_size, seq_len, 2, self.d_k_total)\n",
    "V_proj = V_proj.view(batch_size, seq_len, 2, self.d_v_total)\n",
    "\n",
    "# Split into heads\n",
    "Q = Q_proj.view(batch_size, seq_len, 2, self.n_heads, self.d_k_head)\n",
    "K = K_proj.view(batch_size, seq_len, 2, self.n_heads, self.d_k_head)\n",
    "V = V_proj.view(batch_size, seq_len, 2, self.n_heads, self.d_v_head)\n",
    "\n",
    "# Move real/imaginary index to the end\n",
    "Q = Q.permute(0,1,3,4,2).contiguous()\n",
    "K = K.permute(0,1,3,4,2).contiguous()\n",
    "V = V.permute(0,1,3,4,2).contiguous()\n",
    "\n",
    "#######################################\n",
    "\n",
    "# Optionally, apply complex-valued normalization on inputs (per head)\n",
    "if self.args.use_complex_input_norm == 1: # Normalize query, key, and value\n",
    "    Q_norm = self.cn_q(Q)\n",
    "    K_norm = self.cn_k(K)\n",
    "    V_norm = self.cn_v(V)\n",
    "elif self.args.use_complex_input_norm == 0: # No normalization\n",
    "    Q_norm = Q\n",
    "    K_norm = K\n",
    "    V_norm = V\n",
    "elif self.args.use_complex_input_norm == 2: # Normalize only query and key\n",
    "    Q_norm = self.cn_q(Q)\n",
    "    K_norm = self.cn_k(K)\n",
    "else:\n",
    "    print('Eror: args.use_complex_input_norm must be 0, 1, or 2.')\n",
    "\n",
    "#######################################\n",
    "\n",
    "mu_v, omega_v, lambda_v = compute_lambda(self.mu_v, self.lambda_imag_v, self.args) # Compute eigenvals\n",
    "\n",
    "# Ensure non-negativeness of noise parameters\n",
    "sigma_squared_v = F.softplus(self.sigma_v) # Process noise\n",
    "eta_squared_v = F.softplus(self.eta_v) + self.args.epsilon # Measurement noise\n",
    "gamma_squared_v = F.softplus(self.gamma_v) + self.args.epsilon # Anchor measurement noise\n",
    "\n",
    "if self.args.zero_process_noise == 1:\n",
    "    sigma_squared_v = torch.zeros_like(sigma_squared_v).to(sigma_squared_v.device)\n",
    "if self.args.zero_key_measurement_noise == 1:\n",
    "    eta_squared_v = torch.zeros_like(eta_squared_v).to(eta_squared_v.device)\n",
    "\n",
    "nu = F.softplus(self.nu_sqrt) + self.args.epsilon # Scaling factor\n",
    "\n",
    "#         noise_params = torch.stack((sigma_squared_v, eta_v, gamma_v, nu))\n",
    "#         print(noise_params)\n",
    "#         print('sigma^2 = ', sigma_squared_v.detach().cpu().numpy())\n",
    "#         print('eta^2   = ', eta_squared_v.detach().cpu().numpy())\n",
    "#         print('gamma^2 = ', gamma_squared_v.detach().cpu().numpy())\n",
    "#         print('nu      = ', nu.detach().cpu().numpy())\n",
    "\n",
    "# Get relative time difference\n",
    "t_measure_i = t_measure.squeeze().unsqueeze(1)  # [m, 1]\n",
    "t_measure_j = t_measure.squeeze().unsqueeze(0)  # [1, m]\n",
    "Delta_T = torch.abs(t_measure_i - t_measure_j).unsqueeze(-1)  # [m, m]\n",
    "\n",
    "# Clamp the exponent to ensure safe values\n",
    "exp_rel_v = mu_v * Delta_T\n",
    "exp_rel_safe_v = torch.clamp(exp_rel_v, min=self.args.min_exponent, max=self.args.max_exponent)\n",
    "\n",
    "####################################################\n",
    "\n",
    "Phi_tilde_plus_v, E_rel_v = compute_exp_kernel_isotropic(omega_v, t_measure, exp_rel_safe_v)\n",
    "\n",
    "V_ij_v = compute_covariance_matrix_safe(mu_v, Delta_T, exp_rel_safe_v, sigma_squared_v, eta_squared_v, gamma_squared_v, t_measure, self.args)\n",
    "\n",
    "if self.args.sep_params == 1:\n",
    "    mu_k, omega_k, lambda_k = compute_lambda(self.mu_k, self.lambda_imag_k, self.args)\n",
    "\n",
    "    sigma_squared_k = F.softplus(self.sigma_k) # Process noise\n",
    "    eta_squared_k = F.softplus(self.eta_k) + self.args.epsilon # Measurement noise\n",
    "    gamma_squared_k = F.softplus(self.gamma_k) + self.args.epsilon # Anchor noise\n",
    "\n",
    "    if self.args.zero_process_noise == 1:\n",
    "        sigma_squared_k = torch.zeros_like(sigma_squared_k).to(sigma_squared_k.device)\n",
    "    if self.args.zero_key_measurement_noise == 1:\n",
    "        eta_squared_k = torch.zeros_like(eta_squared_k).to(eta_squared_k.device)\n",
    "\n",
    "    #################\n",
    "\n",
    "    Phi_tilde_plus_k, E_rel_k = compute_exp_kernel_isotropic(omega_k, t_measure, exp_rel_safe_k)\n",
    "\n",
    "    exp_rel_k = mu_k * Delta_T\n",
    "    exp_rel_safe_k = torch.clamp(exp_rel_k, min=self.args.min_exponent, max=self.args.max_exponent)\n",
    "\n",
    "    V_ij_k = compute_covariance_matrix_safe(mu_k, Delta_T, exp_rel_safe_k, sigma_squared_k, eta_squared_k, gamma_squared_k, t_measure, self.args)\n",
    "\n",
    "else:\n",
    "    V_ij_k = V_ij_v\n",
    "    Phi_tilde_plus_k = Phi_tilde_plus_v\n",
    "    E_rel_k = E_rel_v\n",
    "    sigma_squared_k = sigma_squared_v\n",
    "    eta_squared_k = eta_squared_v\n",
    "    gamma_squared_k = gamma_squared_v\n",
    "\n",
    "cos_k = Phi_tilde_plus_k[...,0]\n",
    "sin_k = Phi_tilde_plus_k[...,1]\n",
    "cos_v = Phi_tilde_plus_v[...,0]\n",
    "sin_v = Phi_tilde_plus_v[...,1]\n",
    "\n",
    "Q_tilde = apply_interleaved_rope(Q_norm, cos_k, -sin_k)\n",
    "K_tilde = apply_interleaved_rope(K_norm, cos_k, -sin_k)\n",
    "V_tilde = apply_interleaved_rope(V_norm, cos_v, -sin_v)\n",
    "\n",
    "R_qk_abs_squared = compute_residual_norm_isotropic(Q_tilde, K_tilde, E_rel_k, self.args)\n",
    "\n",
    "A, A_prior, unnormalized_attention = compute_attention_matrix(self, R_qk_abs_squared, V_ij_k, V_ij_v, Delta_T, nu, sigma_squared_k, eta_squared_k, self.args)\n",
    "\n",
    "A_hat = A * E_rel_v # Scale attention weights by the relative decay\n",
    "\n",
    "est_v = compute_estimate(A_hat, V_tilde, Phi_tilde_plus_v, self.args)\n",
    "\n",
    "#############################################\n",
    "\n",
    "# Add Guassian noise to last step (for test-time sampling)\n",
    "if self.args.add_gaussian_noise == 1:\n",
    "    P_tot = torch.sum(unnormalized_attention,-2) # Total precision\n",
    "    V_tot = 1/(P_tot)\n",
    "    V_tot_last = V_tot[:,-1]\n",
    "    V_tot_last = V_tot_last + gamma_squared_v\n",
    "    std_dev = torch.sqrt(V_tot_last/2) # Divide by 2 to deal with complex numbers\n",
    "    gaussian_noise = torch.randn_like(std_dev) * std_dev # Sample from normal distrb\n",
    "    gaussian_noise_unsqueeze = gaussian_noise.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n",
    "    est_v[:, -1:] = est_v[:, -1:] + gaussian_noise_unsqueeze # Add Gaussian noise to last step\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# Add residual connection\n",
    "if self.args.use_inner_residual == 1:\n",
    "    if self.args.use_total_precision_gate == 0: # Simple residual\n",
    "        est_latent = est_v + V # Just add them\n",
    "\n",
    "    elif self.args.use_total_precision_gate == 1: # Precision gate\n",
    "#                P_tot/ (P_tot + P_prior) = 1 / (1 + P_prior/P_tot)\n",
    "#                = 1/(1 + e^{-(ln(P_tot) - ln(P_prior)}) = sigmoid[ln(P_tot) - ln(P_prior)] \n",
    "        P_tot = torch.sum(unnormalized_attention,-2).unsqueeze(-1).unsqueeze(-1)\n",
    "        P_tot_log = torch.log(P_tot)\n",
    "        P_prior_log = torch.log(self.P_prior_param**2 + self.args.epsilon)\n",
    "        g = torch.sigmoid(P_tot_log * self.P_scale**2 - P_prior_log) # Mean precision gate               \n",
    "        est_latent = g * est_v + (1-g) * V # Update using convex combination\n",
    "\n",
    "    elif self.args.use_total_precision_gate == 2: # Learned gate\n",
    "\n",
    "        g = torch.sigmoid(self.P_scale) # Learned gate\n",
    "        est_latent = g * est_v + (1-g) * V # Update using convex combination\n",
    "#                 est_latent = g * est_v + V # No gate on residual connection\n",
    "    else:\n",
    "        print('Error: args.use_total_precision_gate must be 0, 1, or 2.')\n",
    "else:\n",
    "    est_latent = est_v # No residual\n",
    "\n",
    "# Optionally, use complex normalization on outputs\n",
    "if self.args.use_complex_output_norm == 1:\n",
    "    est_norm = self.cn_o(est_latent)\n",
    "else:\n",
    "    est_norm = est_latent\n",
    "\n",
    "# Predict next step using same matrix exponential used for estimation\n",
    "if self.args.compute_next_step_pred == 1:\n",
    "    cos_v_one_step = Phi_tilde_plus_v[1,:,:,0].unsqueeze(0)\n",
    "    sin_v_one_step = Phi_tilde_plus_v[1,:,:,1].unsqueeze(0)\n",
    "    pred_p = torch.exp(mu_v).unsqueeze(-1).unsqueeze(-1) * apply_interleaved_rope(est_norm, cos_v_one_step, sin_v_one_step)\n",
    "\n",
    "else:\n",
    "    pred_p = est_norm\n",
    "\n",
    "# -------------------\n",
    "# Move real/imag dimension back\n",
    "pred_p_permute = pred_p.permute(0,1,4,2,3).contiguous()\n",
    "\n",
    "# Merge heads\n",
    "pred_p_reshape = pred_p_permute.view(batch_size, seq_len, 2, self.d_v_total)     \n",
    "\n",
    "# Stack complex numbers into last dimension\n",
    "pred_p_stack = pred_p_reshape.view(batch_size, 1, seq_len, self.d_v_total*2)\n",
    "\n",
    "#         # Move real/imag dimension back, merge heads, and stack complex numbers into last dimension\n",
    "#         pred_p_stack = pred_p.permute(0, 1, 4, 2, 3).reshape(batch_size, 1, seq_len, -1)\n",
    "\n",
    "# Map back to original basis and get real part\n",
    "out = self.W_o(pred_p_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Model\n",
    "# model_weight_path_full = model_weight_path + 'weights_epoch_' + str(epoch)\n",
    "# model_tensor_path_full = model_tensor_path + 'tensors_epoch_' + str(epoch)\n",
    "# torch.save(model.state_dict(), model_weight_path_full)\n",
    "\n",
    "# tensors_to_save = {\n",
    "# 'epoch_losses': epoch_losses,\n",
    "# 'A_hat_complex': A_hat_complex,\n",
    "# 'epoch_lambdas': epoch_lambdas\n",
    "# }\n",
    "# torch.save(tensors_to_save, model_tensor_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### LOOP FOR REAL/COMPLEX ATTENTION NETS #######\n",
    "\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Training progress...\"):\n",
    "\n",
    "#     epoch_losses = single_epoch_attn(model, train_loader, optimizer, loss, params_list, args)\n",
    "\n",
    "#     # Collect losses\n",
    "#     all_losses[epoch*args.num_its:(epoch+1)*args.num_its] = epoch_losses\n",
    "#     log_mean_epoch_losses[epoch] = np.log(np.mean(epoch_losses))\n",
    "    \n",
    "#     # Visualize results so far:\n",
    "#     if np.mod(epoch+1,args.show_example_epochs) == 0:\n",
    "#         visualize_results_attn(model, train_dataset, all_losses, mean_epoch_losses, log_mean_epoch_losses, all_lambdas, R1, R1i, Pu, Pd, A, epoch, args)\n",
    "# ##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the model autoregressively\n",
    "def autoregressive_sample(model, start_seq, max_gen_len):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        current_seq = start_seq\n",
    "        total_seq = start_seq\n",
    "        \n",
    "        window_size = start_seq.size(2)\n",
    "\n",
    "        for i in range(max_gen_len):\n",
    "            # Forward pass through the model\n",
    "            out, output_dict = model(current_seq)\n",
    "\n",
    "            # Extract the last generated token (the prediction for the next step)\n",
    "            next_token = out[:, :, -1].unsqueeze(2)\n",
    "\n",
    "            # Append new token\n",
    "            total_seq = torch.cat([total_seq, next_token], dim=2)\n",
    "\n",
    "            # Remove first token to keep context window fixed\n",
    "            current_seq = total_seq[:,:,-window_size:]\n",
    "\n",
    "    new_seq = total_seq[:,:,-args.max_gen_len:]\n",
    "\n",
    "    return total_seq, new_seq\n",
    "    \n",
    "args.add_gaussian_noise = 1\n",
    "args.max_gen_len = 100\n",
    "\n",
    "for it, (train_data, _, _, _) in enumerate(train_loader):\n",
    "    start_seq = train_data[:, :-1].unsqueeze(1)\n",
    "    break\n",
    "    \n",
    "total_seq, new_seq = autoregressive_sample(model, start_seq, args.max_gen_len)\n",
    "    \n",
    "traj_tot = torch.matmul(R1i,total_seq.unsqueeze(-1)) # Reverse random mapping\n",
    "X_tot = torch.matmul(Pd,traj_tot) # Map back to lower dim\n",
    "\n",
    "traj_new = torch.matmul(R1i,new_seq.unsqueeze(-1)) # Reverse random mapping\n",
    "X_new = torch.matmul(Pd,traj_new) # Map back to lower dim\n",
    "\n",
    "# Predicted trajectory\n",
    "X_tot_plt = X_tot.squeeze(0)[0].detach().cpu().squeeze().numpy()\n",
    "plt.plot(X_tot_plt.T[0], X_tot_plt.T[1], 'r--', label='Predicted') # Added label\n",
    "\n",
    "X_new_plt = X_new.squeeze(0)[0].detach().cpu().squeeze().numpy()\n",
    "plt.plot(X_new_plt.T[0], X_new_plt.T[1], 'b', label='Predicted') # Added label\n",
    "\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
