{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30dcbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.rc('font', size=20)\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import transformers\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# from collections import Counter\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from tqdm import tqdm # Loading bar\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4807325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from utils import complex_conj_transpose, batched_complex_conj_transpose, complex_exp, complex_exp_v2, complex_hadamard, complex_matmul, complex_division\n",
    "from utils import batched_complex_conj_transpose, batched_complex_hadamard, batched_complex_matmul, batched_complex_division\n",
    "from utils import batched_complex_exp, batched_complex_hadamard_full, batched_complex_matmul_full\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4627f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from dynamics import stochastic_LTI, DynamicSim\n",
    "from dynamics import construct_mapping\n",
    "from dynamics import get_nth_measurement, get_random_measurements\n",
    "from dynamics import linear_spiral, linear_spiral_3D, Lorenz, rand_coupling_matrix, Van_der_Pol_osc\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdcaf091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from precision_attention import compute_residuals, compute_kernel_v1, compute_estimates_and_residuals_vectorized, get_time_diffs, compute_neg_kernel, clamp_exponent_arg\n",
    "from precision_attention import compute_kernel, batched_compute_estimates_and_residuals_vectorized, compute_estimates_and_residuals_irregular_times, compute_nu\n",
    "from precision_attention import compute_precision_v1\n",
    "# from precision_attention import precise_attn, precise_attn_with_correction, precise_attn_full\n",
    "from precision_attention import compute_precision, compute_precision_tanh\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c19e69bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from model import compute_lambda_h\n",
    "from model import init_complex_matrix, build_nearly_identity, initialize_to_correct_model\n",
    "from model import init_weight_masks, apply_weight_masks\n",
    "from model import Complex_MSE_Loss, Batched_Complex_MSE_Loss, inverse_penalty\n",
    "from model import BatchedPrecisionAttentionBlock\n",
    "from model import HadamardLayer, TemporalNorm, TemporalWhiteningLayer\n",
    "from model import PrecisionNet_1layer, PrecisionNet\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4c14d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser('DA')\n",
    "parser.add_argument('--gpu', type=int, default=0) # (Default: 0)\n",
    "args = parser.parse_args(args=[])\n",
    "args.device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
    "print(args.device)\n",
    "\n",
    "torch.manual_seed(2025)\n",
    "np.random.seed(2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acde2d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DataFrame Head (First 5 rows) ---\n",
      "                                                text\n",
      "0                                                   \n",
      "1                              = Robert Boulter = \\n\n",
      "2                                                   \n",
      "3   Robert Boulter is an English film , televisio...\n",
      "4   In 2006 , Boulter starred alongside Whishaw i...\n",
      "\n",
      "--- DataFrame Info (Columns, Non-Null Counts, Dtypes) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44836 entries, 0 to 44835\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    44836 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 350.4+ KB\n",
      "\n",
      "--- DataFrame Description (Statistical Summary) ---\n",
      "         text\n",
      "count   44836\n",
      "unique  26538\n",
      "top          \n",
      "freq    15717\n",
      "\n",
      "--- DataFrame Shape (Rows, Columns) ---\n",
      "Shape: (44836, 1)\n",
      "\n",
      "--- DataFrame Columns ---\n",
      "['text']\n"
     ]
    }
   ],
   "source": [
    "# Visualize data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "local_data_dir = r\"C:\\Users\\Pracioppo\\Desktop\\Peter DynAttn Proj\\data\\wikitext\"\n",
    "\n",
    "# Replace 'your_file.parquet' with the actual path to your Parquet file\n",
    "\n",
    "try:\n",
    "    # Load the Parquet file into a Pandas DataFrame\n",
    "    df = pd.read_parquet(local_data_dir)\n",
    "\n",
    "    print(\"--- DataFrame Head (First 5 rows) ---\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\n--- DataFrame Info (Columns, Non-Null Counts, Dtypes) ---\")\n",
    "    df.info()\n",
    "\n",
    "    print(\"\\n--- DataFrame Description (Statistical Summary) ---\")\n",
    "    print(df.describe()) # For numerical columns\n",
    "\n",
    "    print(f\"\\n--- DataFrame Shape (Rows, Columns) ---\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "\n",
    "    print(f\"\\n--- DataFrame Columns ---\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6547b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-95672dcb03904034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/default to file://C:/Users/Pracioppo/.cache/huggingface/datasets/parquet/default-95672dcb03904034/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0d516cf9584904baf71dd0ee3ba36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712f2efd40b74097b6171738430bbcb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to file://C:/Users/Pracioppo/.cache/huggingface/datasets/parquet/default-95672dcb03904034/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Error loading local files: Loading a dataset cached in a LocalFileSystem is not supported.\n",
      "Please ensure the 'local_data_dir' path is correct and the files are present.\n"
     ]
    }
   ],
   "source": [
    "# # --- Define the local path where you saved the files ---\n",
    "# # IMPORTANT: Replace this with the actual path on your computer!\n",
    "# local_data_dir = r\"C:\\Users\\Pracioppo\\Desktop\\Peter DynAttn Proj\\data\\wikitext\"\n",
    "\n",
    "# # Load the Dataset\n",
    "# local_data_files = {\n",
    "#     'train': f\"{local_data_dir}/train-00000-of-00001.parquet\",\n",
    "#     'validation': f\"{local_data_dir}/validation-00000-of-00001.parquet\",\n",
    "#     'test': f\"{local_data_dir}/test-00000-of-00001.parquet\"\n",
    "# }\n",
    "\n",
    "# try:\n",
    "#     raw_dataset = load_dataset('parquet', data_files=local_data_files)\n",
    "#     print(\"Dataset loaded successfully from local files!\")\n",
    "#     print(raw_dataset)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading local files: {e}\")\n",
    "#     print(\"Please ensure the 'local_data_dir' path is correct and the files are present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae86ea5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load datasets via Pandas workaround...\n",
      "Parquet files loaded into Pandas DataFrames successfully.\n",
      "Pandas DataFrames converted to Hugging Face Dataset objects.\n",
      "\n",
      "Dataset loaded successfully via Pandas workaround!\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Make sure pandas is imported\n",
    "from datasets import Dataset, DatasetDict # Ensure Dataset and DatasetDict are imported\n",
    "\n",
    "# --- Define the local path where you saved the files ---\n",
    "# IMPORTANT: Replace this with the actual path on your computer!\n",
    "local_data_dir = r\"C:\\Users\\Pracioppo\\Desktop\\Peter DynAttn Proj\\data\\wikitext\"\n",
    "\n",
    "# Define the full paths to your parquet files\n",
    "train_file = f\"{local_data_dir}/train-00000-of-00001.parquet\"\n",
    "validation_file = f\"{local_data_dir}/validation-00000-of-00001.parquet\"\n",
    "test_file = f\"{local_data_dir}/test-00000-of-00001.parquet\"\n",
    "\n",
    "try:\n",
    "    print(\"Attempting to load datasets via Pandas workaround...\")\n",
    "\n",
    "    # 1. Load each parquet file into a Pandas DataFrame\n",
    "    train_df = pd.read_parquet(train_file)\n",
    "    validation_df = pd.read_parquet(validation_file)\n",
    "    test_df = pd.read_parquet(test_file)\n",
    "\n",
    "    print(\"Parquet files loaded into Pandas DataFrames successfully.\")\n",
    "\n",
    "    # 2. Convert Pandas DataFrames to Hugging Face Dataset objects\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    validation_dataset = Dataset.from_pandas(validation_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    print(\"Pandas DataFrames converted to Hugging Face Dataset objects.\")\n",
    "\n",
    "    # 3. Create a DatasetDict from these Dataset objects\n",
    "    raw_dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': validation_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "\n",
    "    print(\"\\nDataset loaded successfully via Pandas workaround!\")\n",
    "    print(raw_dataset)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: One or more local files not found in '{local_data_dir}'.\")\n",
    "    print(\"Please ensure the 'local_data_dir' path is correct and the files (train-*.parquet, validation-*.parquet, test-*.parquet) are present.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during Pandas loading: {e}\")\n",
    "    print(\"Please ensure you have pandas and pyarrow installed:\")\n",
    "    print(\"pip install pandas pyarrow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8254690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re # Import regex for more flexible text cleaning\n",
    "# from langdetect import detect, DetectorFactory\n",
    "# DetectorFactory.seed = 0 # for reproducibility\n",
    "\n",
    "def clean_wikitext_examples(examples):\n",
    "    cleaned_texts = []\n",
    "    for text in examples[\"text\"]:\n",
    "        # Step 1: Strip leading/trailing whitespace (including newlines)\n",
    "        stripped_text = text.strip()\n",
    "\n",
    "        # Step 2: Filter out empty strings and Wikipedia-style headings\n",
    "        if not stripped_text or \\\n",
    "           (stripped_text.startswith('=') and stripped_text.endswith('=') and len(stripped_text) < 100) or \\\n",
    "           len(stripped_text) < 100: # Filter very short lines that are likely not useful sentences\n",
    "            continue # Skip this example if it matches criteria\n",
    "\n",
    "        # Step 3: Remove common Wikitext artifacts like '@-@', '= =' using string.replace()\n",
    "        # It's good to replace these with a single space to avoid merging words.\n",
    "        cleaned_text = stripped_text.replace(\" @-@ \", \" \") # Replace \" @-@ \" with a space\n",
    "        cleaned_text = cleaned_text.replace(\" = = \", \" \")  # Replace \" = = \" with a space\n",
    "\n",
    "        # Step 4: Normalize spaces (replace multiple spaces with a single space)\n",
    "        # This is typically done by splitting by space and joining back,\n",
    "        # which also handles leading/trailing spaces if not already stripped.\n",
    "        cleaned_text = ' '.join(cleaned_text.split())\n",
    "\n",
    "        # Step 5: (Optional, more advanced) Language filtering for non-English content.\n",
    "        # This still requires an external library (e.g., `pip install langdetect`).\n",
    "        # from langdetect import detect, DetectorFactory\n",
    "        # DetectorFactory.seed = 0 # for reproducibility\n",
    "        # try:\n",
    "        #     if detect(cleaned_text) != 'en':\n",
    "        #         continue # Skip non-English text\n",
    "        # except Exception: # Handles cases where text is too short for reliable detection\n",
    "        #     pass\n",
    "\n",
    "        if cleaned_text: # Ensure it's not empty after all cleaning steps\n",
    "            cleaned_texts.append(cleaned_text)\n",
    "\n",
    "    return {\"text\": cleaned_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8b423a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b132095053814fb9a6ab4960e4d000bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c67e617f2334cd981f2bd5aae7e6359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5578cedfca4645aa26b1cf7e1706e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_dataset = raw_dataset.map(\n",
    "    clean_wikitext_examples,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"], # The 'text' column will be replaced by the cleaned one\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df90a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train split size: 36718\n",
      "Cleaned train split size: 15283\n",
      "Example cleaned text from train split (first 500 chars):\n",
      "Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real time gameplay as its predecessors , the story runs parallel to the first game and follows th\n",
      "The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original train split size: {len(raw_dataset['train'])}\")\n",
    "print(f\"Cleaned train split size: {len(cleaned_dataset['train'])}\")\n",
    "print(f\"Example cleaned text from train split (first 500 chars):\")\n",
    "print(cleaned_dataset['train'][0]['text'][:500]) # This should now be actual content\n",
    "print(cleaned_dataset['train'][1]['text'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f76dc6d",
   "metadata": {},
   "source": [
    "### From txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# import os # For creating/cleaning up the dummy file\n",
    "\n",
    "# \"\"\"\n",
    "# Loads a text file, removes words that are entirely in uppercase,\n",
    "# and breaks the processed text into sections of 500 words.\n",
    "\n",
    "# Args:\n",
    "#     file_path (str): The path to the input text file.\n",
    "\n",
    "# Returns:\n",
    "#     list: A list of strings, where each string is a section of up to 500 words.\n",
    "#           Returns an empty list if the file is not found or an error occurs.\n",
    "# \"\"\"\n",
    "\n",
    "# txt_dir = r\"C:\\Users\\Pracioppo\\Desktop\\Peter DynAttn Proj\\data\\poets\"\n",
    "\n",
    "# file_path = txt_dir + \"\\\\\" + \"milton\" + \".txt\"\n",
    "\n",
    "# file_path\n",
    "\n",
    "# # 1. Load the text file\n",
    "# try:\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         full_text = f.read()\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Error: The file '{file_path}' was not found.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred while reading the file: {e}\")\n",
    "\n",
    "# # 2. Remove all words that are entirely in uppercase\n",
    "# # Split the full text into individual words based on whitespace\n",
    "# words = full_text.split()\n",
    "# filtered_words = []\n",
    "# for word in words:\n",
    "#     # Create a version of the word to check for all-caps property.\n",
    "#     # This involves stripping punctuation from its ends, so \"WORD!\" becomes \"WORD\".\n",
    "#     word_for_check = word.strip(string.punctuation)\n",
    "\n",
    "#     # Check if the stripped word consists only of alphabetic characters\n",
    "#     # AND if all those alphabetic characters are uppercase.\n",
    "#     # This prevents numbers, symbols, or mixed-case words from being\n",
    "#     # incorrectly removed (e.g., \"123ABC\" or \"Python\").\n",
    "#     if word_for_check.isalpha() and word_for_check.isupper():\n",
    "#         # If it's an all-caps alphabetic word, we skip it (don't add to filtered_words)\n",
    "#         continue\n",
    "#     else:\n",
    "#         # Otherwise, keep the original word (with its punctuation)\n",
    "#         filtered_words.append(word)\n",
    "\n",
    "# # Join the filtered words back into a single string\n",
    "# cleaned_text = ' '.join(filtered_words)\n",
    "\n",
    "# # 3. Break the cleaned text into sections of 500 words\n",
    "# # First, split the cleaned text back into a list of words.\n",
    "# cleaned_words_list = cleaned_text.split()\n",
    "\n",
    "# # sections = []\n",
    "# # # Iterate through the list, taking chunks of 500 words\n",
    "# # for i in range(0, len(cleaned_words_list), 500):\n",
    "# #     section_words = cleaned_words_list[i : i + 500] # Slice the list\n",
    "# #     sections.append(' '.join(section_words)) # Join the words in the section back into a string\n",
    "\n",
    "# # len(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7533c15",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12921197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# local_bert_dir = r\"C:\\Users\\Pracioppo\\Desktop\\Peter DynAttn Proj\\data\\local_bert_dir\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(local_bert_dir)\n",
    "# print(\"Tokenizer loaded!\")\n",
    "\n",
    "local_bert_mini_dir = r\"C:\\Users\\Pracioppo\\Desktop\\Peter DynAttn Proj\\data\\prajjwal1_bert_mini\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_bert_mini_dir)\n",
    "print(\"Tokenizer loaded!\")\n",
    "\n",
    "# Ensure tokenizer has a pad_token if your model or DataCollator needs it\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    print(\"Added padding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6119fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # This function uses the 'tokenizer' object defined globally above\n",
    "    return tokenizer(examples[\"text\"]) # No truncation/max_length here for LM concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1f66341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing cleaned dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283bece36f1341548ab54a0829a40b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd34ae7851544bbc87a3a38e1fb2b5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2e720a119d46bf950920da9ed9ecec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply Tokenization (Corrected map call)\n",
    "print(\"\\nTokenizing cleaned dataset...\")\n",
    "tokenized_dataset = cleaned_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"], # This removes the original 'text' column after tokenization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32f2b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Block Size for Language Modeling\n",
    "block_size = 128\n",
    "\n",
    "# --- 8. Define Grouping Function for Language Modeling ---\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af49f085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grouping texts into blocks of size 128 for language modeling...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b364c66fb04f96b341b62763c388cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100aaf674b5441199f9ea0f3569182ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fe8e95f8534a31a85a20d241ee0121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts grouped for language modeling.\n"
     ]
    }
   ],
   "source": [
    "# Apply Grouping\n",
    "print(f\"\\nGrouping texts into blocks of size {block_size} for language modeling...\")\n",
    "lm_dataset = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True\n",
    ")\n",
    "lm_dataset.set_format(\"torch\")\n",
    "print(\"Texts grouped for language modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f46ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y transformers datasets huggingface_hub tokenizers\n",
    "\n",
    "# !pip install transformers==4.25.0 datasets==2.5.0 huggingface_hub==0.10.0 tokenizers==0.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6cfb84f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\Pracioppo\\Desktop\\Peter DynAttn Proj\\data\\prajjwal1_bert_mini were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(local_bert_mini_dir).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "856c5414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing single batch (iteration 0) ---\n",
      "Shape of input_ids in this batch: torch.Size([16, 128])\n",
      "Shape of attention_mask in this batch: torch.Size([16, 128])\n",
      "Shape of token_type_ids in this batch: torch.Size([16, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 256])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(lm_dataset[\"train\"], shuffle=True, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(lm_dataset[\"validation\"], batch_size=batch_size)\n",
    "test_dataloader = DataLoader(lm_dataset[\"test\"], batch_size=batch_size)\n",
    "\n",
    "for it, sample in enumerate(train_dataloader):\n",
    "    print(f\"\\n--- Processing single batch (iteration {it}) ---\")\n",
    "\n",
    "    # 1. Extract input tensors from the 'sample' dictionary\n",
    "    #    These are already PyTorch tensors due to lm_dataset.set_format(\"torch\")\n",
    "    input_ids = sample['input_ids']\n",
    "    attention_mask = sample['attention_mask']\n",
    "    token_type_ids = sample['token_type_ids'] # Essential for BERT-like models\n",
    "\n",
    "    print(f\"Shape of input_ids in this batch: {input_ids.shape}\")\n",
    "    print(f\"Shape of attention_mask in this batch: {attention_mask.shape}\")\n",
    "    print(f\"Shape of token_type_ids in this batch: {token_type_ids.shape}\")\n",
    "\n",
    "    # 2. Move these input tensors to the same device as your model (GPU or CPU)\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    token_type_ids = token_type_ids.to(device)\n",
    "\n",
    "    # 3. Pass inputs through the model to get embeddings\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference (saves memory, speeds up)\n",
    "        model_output = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "    # 4. Access the contextualized embeddings (the last hidden state)\n",
    "    contextual_embeddings_for_this_batch = model_output.last_hidden_state\n",
    "    \n",
    "    break\n",
    "    \n",
    "with torch.no_grad(): # No need to compute gradients\n",
    "    # This calls the forward method of the BertEmbeddings module\n",
    "    # It takes input_ids, token_type_ids, and internally adds positional embeddings,\n",
    "    # then applies LayerNorm and Dropout.\n",
    "    initial_input_embeddings = model.embeddings(\n",
    "        input_ids=input_ids,\n",
    "        token_type_ids=token_type_ids # Pass token_type_ids for BERT models\n",
    "    )\n",
    "    \n",
    "initial_input_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e6530b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_similarity(query_emb, vocab_embs, alpha = 1):\n",
    "#     \"\"\"\n",
    "#     Calculates custom similarity\n",
    "#     \"\"\"\n",
    "\n",
    "#     diff = query_emb.unsqueeze(0) - vocab_embs\n",
    "#     p = 1/(alpha + diff**2)\n",
    "    \n",
    "#     scores = torch.mean(p,axis=1)\n",
    "    \n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a1b04bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: '.', Similarity: 0.6112\n"
     ]
    }
   ],
   "source": [
    "def embedding_to_nearest_token(vocab_embed, query_embed):\n",
    "    \"\"\"\n",
    "    Given an embedding, find the nearest embedding in the vocabulary and output the corresponding token.\n",
    "    \"\"\"\n",
    "\n",
    "    similarities = F.cosine_similarity(query_embed.unsqueeze(0), vocab_embeddings, dim=1)\n",
    "\n",
    "    # Find the top K similarities and their indices\n",
    "    top_k_similarities, top_k_indices = torch.topk(similarities, k=1)\n",
    "\n",
    "    # Get the token ID\n",
    "    token_id = top_k_indices.item()\n",
    "    # Convert the token ID back to a readable token string\n",
    "    token_string = tokenizer.convert_ids_to_tokens(token_id)\n",
    "    similarity_score = top_k_similarities.item()\n",
    "\n",
    "    return token_string, similarity_score\n",
    "\n",
    "query_embed = initial_input_embeddings[0,0]\n",
    "vocab_embed = model.embeddings.word_embeddings.weight\n",
    "\n",
    "token_string, similarity_score = embedding_to_nearest_token(vocab_embed, query_embed)\n",
    "\n",
    "print(f\"Token: '{token_string}', Similarity: {similarity_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
