{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e82d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# from xpos_relative_position import XPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dddb72c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2022 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def fixed_pos_embedding(x):\n",
    "    seq_len, dim = x.shape\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(0, dim) / dim))\n",
    "    sinusoid_inp = (\n",
    "        torch.einsum(\"i , j -> i j\", torch.arange(0, seq_len, dtype=torch.float), inv_freq).to(x)\n",
    "    )\n",
    "    return torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)\n",
    "\n",
    "def rotate_every_two(x):\n",
    "    x1 = x[:, :, ::2]\n",
    "    x2 = x[:, :, 1::2]\n",
    "    x = torch.stack((-x2, x1), dim=-1)\n",
    "    return x.flatten(-2)  # in einsum notation: rearrange(x, '... d j -> ... (d j)')\\\n",
    "\n",
    "def duplicate_interleave(m):\n",
    "    \"\"\"\n",
    "    A simple version of `torch.repeat_interleave` for duplicating a matrix while interleaving the copy.\n",
    "    \"\"\"\n",
    "    dim0 = m.shape[0]\n",
    "    m = m.view(-1, 1)  # flatten the matrix\n",
    "    m = m.repeat(1, 2)  # repeat all elements into the 2nd dimension\n",
    "    m = m.view(dim0, -1)  # reshape into a matrix, interleaving the copy\n",
    "    return m\n",
    "\n",
    "def apply_rotary_pos_emb(x, sin, cos, scale=1):\n",
    "    sin, cos = map(lambda t: duplicate_interleave(t * scale), (sin, cos))\n",
    "    # einsum notation for lambda t: repeat(t[offset:x.shape[1]+offset,:], \"n d -> () n () (d j)\", j=2)\n",
    "    return (x * cos) + (rotate_every_two(x) * sin)\n",
    "\n",
    "\n",
    "class XPOS(nn.Module):\n",
    "    def __init__(\n",
    "        self, head_dim, scale_base=512\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.scale_base = scale_base\n",
    "        self.register_buffer(\n",
    "            \"scale\", (torch.arange(0, head_dim, 2) + 0.4 * head_dim) / (1.4 * head_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, offset=0, downscale=False):\n",
    "        length = x.shape[1]\n",
    "        min_pos = -(length + offset) // 2\n",
    "        max_pos = length + offset + min_pos\n",
    "        scale = self.scale ** torch.arange(min_pos, max_pos, 1).to(self.scale).div(self.scale_base)[:, None]\n",
    "        sin, cos = fixed_pos_embedding(scale)\n",
    "\n",
    "        if scale.shape[0] > length:\n",
    "            scale = scale[-length:]\n",
    "            sin = sin[-length:]\n",
    "            cos = cos[-length:]\n",
    "        \n",
    "        if downscale:\n",
    "            scale = 1 / scale\n",
    "\n",
    "        x = apply_rotary_pos_emb(x, sin, cos, scale)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad3f8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRetention(nn.Module):\n",
    "    def __init__(self, hidden_size, gamma, head_size=None, double_v_dim=False):\n",
    "        \"\"\"\n",
    "        Simple retention mechanism based on the paper\n",
    "        \"Retentive Network: A Successor to Transformer for Large Language Models\"[https://arxiv.org/pdf/2307.08621.pdf]\n",
    "        \"\"\"\n",
    "        super(SimpleRetention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        if head_size is None:\n",
    "            head_size = hidden_size\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.v_dim = head_size * 2 if double_v_dim else head_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.W_Q = nn.Parameter(torch.randn(hidden_size, head_size) / hidden_size)\n",
    "        self.W_K = nn.Parameter(torch.randn(hidden_size, head_size) / hidden_size)\n",
    "        self.W_V = nn.Parameter(torch.randn(hidden_size, self.v_dim) / hidden_size)\n",
    "        \n",
    "        self.xpos = XPOS(head_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Parallel (default) representation of the retention mechanism.\n",
    "        X: (batch_size, sequence_length, hidden_size)\n",
    "        \"\"\"\n",
    "        sequence_length = X.shape[1]\n",
    "        D = self._get_D(sequence_length).to(self.W_Q.device)\n",
    "\n",
    "        Q = (X @ self.W_Q)\n",
    "        K = (X @ self.W_K)\n",
    "\n",
    "        Q = self.xpos(Q)\n",
    "        K = self.xpos(K, downscale=True)\n",
    "\n",
    "        V = X @ self.W_V\n",
    "        ret = (Q @ K.permute(0, 2, 1)) * D.unsqueeze(0)\n",
    "        \n",
    "        return ret @ V\n",
    "        \n",
    "    def forward_recurrent(self, x_n, s_n_1, n):\n",
    "        \"\"\"\n",
    "        Recurrent representation of the retention mechanism.\n",
    "        x_n: (batch_size, 1, hidden_size)\n",
    "        s_n_1: (batch_size, hidden_size, v_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        Q = (x_n @ self.W_Q)\n",
    "        K = (x_n @ self.W_K)\n",
    "\n",
    "        Q = self.xpos(Q, n+1)\n",
    "        K = self.xpos(K, n+1, downscale=True)\n",
    "\n",
    "        V = x_n @ self.W_V\n",
    "\n",
    "        # K: (batch_size, 1, hidden_size)\n",
    "        # V: (batch_size, 1, v_dim)\n",
    "        # s_n = gamma * s_n_1 + K^T @ V\n",
    "\n",
    "        s_n = self.gamma * s_n_1 + (K.transpose(-1, -2) @ V)\n",
    "        \n",
    "        return (Q @ s_n), s_n\n",
    "    \n",
    "    def forward_chunkwise(self, x_i, r_i_1, i):\n",
    "        \"\"\"\n",
    "        Chunkwise representation of the retention mechanism.\n",
    "        x_i: (batch_size, chunk_size, hidden_size)\n",
    "        r_i_1: (batch_size, hidden_size, v_dim)\n",
    "        \"\"\"\n",
    "        batch, chunk_size, _ = x_i.shape\n",
    "        D = self._get_D(chunk_size)\n",
    "\n",
    "        Q = (x_i @ self.W_Q)\n",
    "        K = (x_i @ self.W_K)\n",
    "\n",
    "        Q = self.xpos(Q, i * chunk_size)\n",
    "        K = self.xpos(K, i * chunk_size, downscale=True)\n",
    "\n",
    "        V = x_i @ self.W_V\n",
    "        \n",
    "        r_i =(K.transpose(-1, -2) @ (V * D[-1].view(1, chunk_size, 1))) + (self.gamma ** chunk_size) * r_i_1\n",
    "\n",
    "        inner_chunk = ((Q @ K.transpose(-1, -2)) * D.unsqueeze(0)) @ V\n",
    "        \n",
    "        #e[i,j] = gamma ** (i+1)\n",
    "        e = torch.zeros(batch, chunk_size, 1)\n",
    "        \n",
    "        for _i in range(chunk_size):\n",
    "            e[:, _i, :] = self.gamma ** (_i + 1)\n",
    "        \n",
    "        cross_chunk = (Q @ r_i_1) * e\n",
    "        \n",
    "        return inner_chunk + cross_chunk, r_i\n",
    "\n",
    "    def _get_D(self, sequence_length):\n",
    "        n = torch.arange(sequence_length).unsqueeze(1)\n",
    "        m = torch.arange(sequence_length).unsqueeze(0)\n",
    "\n",
    "        # Broadcast self.gamma ** (n - m) with appropriate masking to set values where n < m to 0\n",
    "        D = (self.gamma ** (n - m)) * (n >= m).float()  #this results in some NaN when n is much larger than m\n",
    "        # fill the NaN with 0\n",
    "        D[D != D] = 0\n",
    "\n",
    "        return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fcea352",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleRetention(nn.Module):\n",
    "    def __init__(self, hidden_size, heads, double_v_dim=False):\n",
    "        \"\"\"\n",
    "        Multi-scale retention mechanism based on the paper\n",
    "        \"Retentive Network: A Successor to Transformer for Large Language Models\"[https://arxiv.org/pdf/2307.08621.pdf]\n",
    "        \"\"\"\n",
    "        super(MultiScaleRetention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.v_dim = hidden_size * 2 if double_v_dim else hidden_size\n",
    "        self.heads = heads\n",
    "        assert hidden_size % heads == 0, \"hidden_size must be divisible by heads\"\n",
    "        self.head_size = hidden_size // heads\n",
    "        self.head_v_dim = hidden_size * 2 if double_v_dim else hidden_size\n",
    "        \n",
    "        self.gammas = (1 - torch.exp(torch.linspace(math.log(1/32), math.log(1/512), heads))).detach().cpu().tolist()\n",
    "\n",
    "        self.swish = lambda x: x * torch.sigmoid(x)\n",
    "        self.W_G = nn.Parameter(torch.randn(hidden_size, self.v_dim) / hidden_size)\n",
    "        self.W_O = nn.Parameter(torch.randn(self.v_dim, hidden_size) / hidden_size)\n",
    "        self.group_norm = nn.GroupNorm(heads, self.v_dim)\n",
    "\n",
    "        self.retentions = nn.ModuleList([\n",
    "            SimpleRetention(self.hidden_size, gamma, self.head_size, double_v_dim) for gamma in self.gammas\n",
    "        ])\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        parallel representation of the multi-scale retention mechanism\n",
    "        \"\"\"\n",
    "\n",
    "        # apply each individual retention mechanism to X\n",
    "        Y = []\n",
    "        for i in range(self.heads):\n",
    "            Y.append(self.retentions[i](X))\n",
    "        \n",
    "        Y = torch.cat(Y, dim=2)\n",
    "        Y_shape = Y.shape\n",
    "        Y = self.group_norm(Y.reshape(-1, self.v_dim)).reshape(Y_shape)\n",
    "\n",
    "        return (self.swish(X @ self.W_G) * Y) @ self.W_O\n",
    "    \n",
    "    def forward_recurrent(self, x_n, s_n_1s, n):\n",
    "        \"\"\"\n",
    "        recurrent representation of the multi-scale retention mechanism\n",
    "        x_n: (batch_size, 1, hidden_size)\n",
    "        s_n_1s: (batch_size, heads, head_size, head_size)\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "        # apply each individual retention mechanism to a slice of X\n",
    "        Y = []\n",
    "        s_ns = []\n",
    "        for i in range(self.heads):\n",
    "            y, s_n = self.retentions[i].forward_recurrent(\n",
    "                x_n[:, :, :], s_n_1s[i], n\n",
    "                )\n",
    "            Y.append(y)\n",
    "            s_ns.append(s_n)\n",
    "        \n",
    "        Y = torch.cat(Y, dim=2)\n",
    "        Y_shape = Y.shape\n",
    "        Y = self.group_norm(Y.reshape(-1, self.v_dim)).reshape(Y_shape)\n",
    "        \n",
    "        return (self.swish(x_n @ self.W_G) * Y) @ self.W_O, s_ns\n",
    "\n",
    "    def forward_chunkwise(self, x_i, r_i_1s, i):\n",
    "        \"\"\"\n",
    "        chunkwise representation of the multi-scale retention mechanism\n",
    "        x_i: (batch_size, chunk_size, hidden_size)\n",
    "        r_i_1s: (batch_size, heads, head_size, head_size)\n",
    "        \"\"\"\n",
    "        batch, chunk_size, _ = x_i.shape\n",
    "\n",
    "        # apply each individual retention mechanism to a slice of X\n",
    "        Y = []\n",
    "        r_is = []\n",
    "        for j in range(self.heads):\n",
    "            y, r_i = self.retentions[j].forward_chunkwise(\n",
    "                x_i[:, :, :], r_i_1s[j], i\n",
    "                )\n",
    "            Y.append(y)\n",
    "            r_is.append(r_i)\n",
    "        \n",
    "        \n",
    "        Y = torch.cat(Y, dim=2)\n",
    "        Y_shape = Y.shape\n",
    "        Y = self.group_norm(Y.reshape(-1, self.v_dim)).reshape(Y_shape)\n",
    "\n",
    "        return (self.swish(x_i @ self.W_G) * Y) @ self.W_O, r_is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a07e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
